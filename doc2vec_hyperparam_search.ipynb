{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10136e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train: 25000, num test: 25000\n",
      "tot num reviews: 50000\n",
      "num train: 1800, num test: 200\n",
      "tot num reviews: 2000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run a hyperparameter search over the doc2vec model! \n",
    "'''\n",
    "\n",
    "from Corpora import MovieReviewCorpus\n",
    "from Lexicon import SentimentLexicon\n",
    "from Statistics import SignTest\n",
    "from Classifiers import NaiveBayesText, SVMText\n",
    "from Extensions import SVMDoc2Vec, DocFeaturizer\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "ax_size = 16\n",
    "title_size=18\n",
    "\n",
    "save_dir = \"doc2vec_models/\"\n",
    "import os\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "good = [\"good\", \"great\",\"amazing\",\"best\"]\n",
    "bad = [\"avoid\",\"boring\",\"terrible\",\"bad\"]\n",
    "extreme = [\"incredibly\", \"particularly\", \"very\"] # e.g., extreme words, or neutral\n",
    "neutral = [\"the\", \"a\", \"but\"]\n",
    "\n",
    "# model_pth= f\"{save_dir}sample_model.pkl\"\n",
    "# DF.model.save(model_pth)\n",
    "\n",
    "corpus=MovieReviewCorpus(stemming=False,pos=False,use_imdb=True)\n",
    "mini_corpus=MovieReviewCorpus(stemming=False,pos=False,use_imdb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e34370bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_dims = [10, 50, 100, 200, 400]\n",
    "# windows=[3,5,10]\n",
    "# accs = np.zeros([len(feature_dims), len(windows)])\n",
    "# np.savetxt(\"currentState.txt\", accs)\n",
    "# accs2 = np.loadtxt(\"currentState.txt\")\n",
    "# accs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f8886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  10  window:  3\n",
      "kernel:  rbf\n",
      "kernel:  rbf\n",
      "Acc:  0.705\n",
      "features:  10  window:  5\n",
      "kernel:  rbf\n",
      "kernel:  rbf\n",
      "Acc:  0.615\n",
      "features:  10  window:  10\n",
      "kernel:  rbf\n",
      "kernel:  rbf\n",
      "Acc:  0.685\n",
      "features:  50  window:  3\n",
      "kernel:  rbf\n",
      "kernel:  rbf\n",
      "Acc:  0.67\n",
      "features:  50  window:  5\n",
      "kernel:  rbf\n",
      "kernel:  rbf\n",
      "Acc:  0.63\n",
      "features:  50  window:  10\n",
      "kernel:  rbf\n",
      "kernel:  rbf\n",
      "Acc:  0.695\n",
      "features:  100  window:  3\n",
      "kernel:  rbf\n",
      "kernel:  rbf\n",
      "Acc:  0.62\n",
      "features:  100  window:  5\n",
      "kernel:  rbf\n",
      "kernel:  rbf\n",
      "Acc:  0.645\n",
      "features:  100  window:  10\n",
      "kernel:  rbf\n",
      "kernel:  rbf\n",
      "Acc:  0.66\n",
      "features:  200  window:  3\n",
      "kernel:  rbf\n",
      "kernel:  rbf\n",
      "Acc:  0.615\n",
      "features:  200  window:  5\n"
     ]
    }
   ],
   "source": [
    "feature_dims = [10, 50, 100, 200, 400]\n",
    "windows=[3,5,10] # original paper looked at 10 and 400\n",
    "# min_counts=[2,5]\n",
    "# dm_concats=[0,1]\n",
    "min_count = 2 \n",
    "dm_concat = 1\n",
    "epochs=30 # for hyperparam search\n",
    "inf_epochs = 10\n",
    "accs = np.zeros([len(feature_dims), len(windows)])\n",
    "for i, feature_dim in enumerate(feature_dims):\n",
    "    for j, window in enumerate(windows): \n",
    "        print(\"features: \", feature_dim, \" window: \", window)\n",
    "        DF = DocFeaturizer(feature_dim, window, dm=1, dbow_words=0, \n",
    "                           dm_concat=dm_concat,min_count=min_count)\n",
    "        DF.train_model(corpus.unsup_train, epochs)\n",
    "        SVMDV=SVMDoc2Vec(model=DF,bigrams=False,trigrams=False,discard_closed_class=False,\n",
    "                        inf_epochs=inf_epochs)\n",
    "        SVMDV.train(mini_corpus.train) \n",
    "        SVMDV.test(mini_corpus.test)\n",
    "        acc= SVMDV.getAccuracy()\n",
    "        accs[i,j] = acc\n",
    "        print(\"Acc: \", acc)\n",
    "        np.savetxt(\"currentState.txt\",accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fb7498",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(accs,xticklabels=[str(window) for window in windows], yticklabels=[str(feature_dim) for feature_dim in feature_dims],cmap=\"coolwarm\",ax=ax)\n",
    "ax.set_xlabel(\"Window Size\",fontsize=ax_size)\n",
    "ax.set_ylabel(\"Feature Dimensionality\",fontsize=ax_size)\n",
    "ax.set_title(f\"PV-DM: Feature Dimensionality vs. Window Size\",fontsize=title_size) \n",
    "plt.savefig(f\"featureDimWindowSize.png\", dpi=400, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bbd37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 200\n",
    "windows=5\n",
    "dm_concats = [0,1]\n",
    "dbow_words_vals = [0,1]\n",
    "# dm_means = [0,1]\n",
    "epochs=30 # for hyperparam search\n",
    "inf_epochs = 10\n",
    "accs2 = np.zeros([len(dm_concats), len(dbow_words_vals)])\n",
    "for i, dm_concat in enumerate(dm_concats):\n",
    "    for j, dbow_words in enumerate(dbow_words_vals): \n",
    "        print(\"dm concat: \", dm_concat, \" window: \", window)\n",
    "        DF = DocFeaturizer(feature_dim, window, dm=0, dbow_words=dbow_words, \n",
    "                           dm_concat=dm_concat,min_count=min_count)\n",
    "        DF.train_model(corpus.unsup_train, epochs)\n",
    "        SVMDV=SVMDoc2Vec(model=DF,bigrams=False,trigrams=False,discard_closed_class=False,\n",
    "                        inf_epochs=inf_epochs)\n",
    "        SVMDV.train(mini_corpus.train) \n",
    "        SVMDV.test(mini_corpus.test)\n",
    "        acc= SVMDV.getAccuracy()\n",
    "        accs2[i,j] = acc\n",
    "        print(\"Acc: \", accs2)\n",
    "        np.savetxt(\"currentState_search2.txt\",accs2)\n",
    "        \n",
    "# TODO: port this data in to a table!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac82447",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dims = [10, 50, 100, 200, 400]\n",
    "windows=[3,5,10] # original paper looked at 10 and 400\n",
    "# min_counts=[2,5]\n",
    "# dm_concats=[0,1]\n",
    "min_count = 2 \n",
    "dm_concat = 1\n",
    "epochs=50 # for hyperparam search\n",
    "inf_epochs = 30\n",
    "accs = np.zeros([len(feature_dims), len(windows)])\n",
    "for i, feature_dim in enumerate(feature_dims):\n",
    "    for j, window in enumerate(windows): \n",
    "        print(\"features: \", feature_dim, \" window: \", window)\n",
    "        DF = DocFeaturizer(feature_dim, window, dm=1, dbow_words=0, \n",
    "                           dm_concat=dm_concat,min_count=min_count)\n",
    "        DF.train_model(corpus.unsup_train, epochs)\n",
    "        SVMDV=SVMDoc2Vec(model=DF,bigrams=False,trigrams=False,discard_closed_class=False,\n",
    "                        inf_epochs=inf_epochs)\n",
    "        SVMDV.train(mini_corpus.train) \n",
    "        SVMDV.test(mini_corpus.test)\n",
    "        acc= SVMDV.getAccuracy()\n",
    "        accs[i,j] = acc\n",
    "        print(\"Acc: \", acc)\n",
    "        np.savetxt(\"currentState.txt\",accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ab81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(accs,xticklabels=[str(window) for window in windows], yticklabels=[str(feature_dim) for feature_dim in feature_dims],cmap=\"coolwarm\",ax=ax)\n",
    "ax.set_xlabel(\"Window Size\",fontsize=ax_size)\n",
    "ax.set_ylabel(\"Feature Dimensionality\",fontsize=ax_size)\n",
    "ax.set_title(f\"PV-DM: Feature Dimensionality vs. Window Size\",fontsize=title_size) \n",
    "plt.savefig(f\"featureDimWindowSize2.png\", dpi=400, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26bc9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dims = [10, 50, 100, 200, 400]\n",
    "windows=[3,5,10] # original paper looked at 10 and 400\n",
    "# min_counts=[2,5]\n",
    "# dm_concats=[0,1]\n",
    "min_count = 2 \n",
    "dm_concat = 1\n",
    "epochs=50 # for hyperparam search\n",
    "inf_epochs = 30\n",
    "accs3 = np.zeros([len(feature_dims), len(windows)])\n",
    "for i, feature_dim in enumerate(feature_dims):\n",
    "    for j, window in enumerate(windows): \n",
    "        print(\"features: \", feature_dim, \" window: \", window)\n",
    "        DF = DocFeaturizer(feature_dim, window, dm=0, dbow_words=1, \n",
    "                           dm_concat=dm_concat,min_count=min_count)\n",
    "        DF.train_model(corpus.unsup_train, epochs)\n",
    "        SVMDV=SVMDoc2Vec(model=DF,bigrams=False,trigrams=False,discard_closed_class=False,\n",
    "                        inf_epochs=inf_epochs)\n",
    "        SVMDV.train(mini_corpus.train) \n",
    "        SVMDV.test(mini_corpus.test)\n",
    "        acc= SVMDV.getAccuracy()\n",
    "        accs3[i,j] = acc\n",
    "        print(\"Acc: \", acc)\n",
    "        np.savetxt(\"currentState_dbow.txt\",accs3)\n",
    "        \n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(accs3,xticklabels=[str(window) for window in windows], yticklabels=[str(feature_dim) for feature_dim in feature_dims],cmap=\"coolwarm\",ax=ax)\n",
    "ax.set_xlabel(\"Window Size\",fontsize=ax_size)\n",
    "ax.set_ylabel(\"Feature Dimensionality\",fontsize=ax_size)\n",
    "ax.set_title(f\"PV-DM: Feature Dimensionality vs. Window Size\",fontsize=title_size) \n",
    "plt.savefig(f\"featureDimWindowSize3.png\", dpi=400, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feature_dim = 100\n",
    "# window = 3\n",
    "# epochs = 50\n",
    "# DF = DocFeaturizer(feature_dim, window, dm=0, dbow_words=1, dm_concat=0)\n",
    "# # DF.train_model(corpus.train, epochs)\n",
    "# # SVMDV=SVMDoc2Vec(model=DF,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "# # SVMDV.train(corpus.train)\n",
    "\n",
    "# model = Doc2Vec.load(f\"{save_dir}mini_data_model.pkl\")\n",
    "# wv = model.wv\n",
    "# dv= model.dv\n",
    "# vocab = list(model.wv.index_to_key)\n",
    "# X = wv[vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0741d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install testfixtures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help from: https://github.com/RaRe-Technologies/gensim/blob/3c3506d51a2caf6b890de3b1b32a8b85f7566ca5/docs/notebooks/doc2vec-IMDB.ipynb\n",
    "# cite original paper showing combined = better\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe4964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "analyses to run \n",
    "vector addition (see pg 4 of orig doc2vec paper -- analogies) ex. pv(lady gaga) - wv(american) + wv(japanese) = japanese version of lady gaga\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136ff3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6405469",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
    "fig.suptitle('Class Separation of Documents', fontsize=title_size + 3)\n",
    "\n",
    "corpus_to_plot = mini_corpus # TODO: change to be the IMDB corpus!!!!\n",
    "\n",
    "labels = [label for label, _ in corpus_to_plot.train]\n",
    "dv = model.dv\n",
    "doc_ids = list(range(len(dv)))\n",
    "X_docs = dv[doc_ids]\n",
    "neg_ids = labels.index(\"NEG\")\n",
    "\n",
    "ax = axes[0]\n",
    "\n",
    "pca_docs = PCA(n_components=2)\n",
    "X_pca_docs = pca_docs.fit_transform(X_docs)\n",
    "# plot positive and negative separately\n",
    "ax.scatter(X_pca_docs[:neg_ids, 0], X_pca_docs[:neg_ids, 1],label=\"POS\")\n",
    "ax.scatter(X_pca_docs[neg_ids:, 0], X_pca_docs[neg_ids:, 1],label=\"NEG\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"PC 1\",fontsize=ax_size)\n",
    "ax.set_ylabel(\"PC 2\",fontsize=ax_size)\n",
    "ax.set_title(f\"PCA\",fontsize=title_size) \n",
    "ax.legend()\n",
    "\n",
    "\n",
    "ax = axes[1]\n",
    "\n",
    "tsne_docs = TSNE(n_components=2)\n",
    "X_tsne_docs = tsne_docs.fit_transform(X_docs)\n",
    "# plot positive and negative separately\n",
    "ax.scatter(X_tsne_docs[:neg_ids, 0], X_tsne_docs[:neg_ids, 1],label=\"POS\")\n",
    "ax.scatter(X_tsne_docs[neg_ids:, 0], X_tsne_docs[neg_ids:, 1],label=\"NEG\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Dim 1\",fontsize=ax_size)\n",
    "ax.set_ylabel(\"Dim 2\",fontsize=ax_size)\n",
    "ax.set_title(f\"t-SNE\",fontsize=title_size) \n",
    "ax.legend()\n",
    "\n",
    "plt.savefig(f\"dimRedIMDB.png\", dpi=400, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b83f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sample_words = good + neutral + extreme + bad\n",
    "V = len(sample_words)\n",
    "sims = np.zeros([V, V])\n",
    "for i, w_i in enumerate(sample_words):\n",
    "    for j, w_j in enumerate(sample_words):\n",
    "        sims[i][j] = wv.similarity(w_i, w_j)\n",
    "        \n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(sims,xticklabels=sample_words, yticklabels=sample_words,cmap=\"coolwarm\",ax=ax)\n",
    "# ax.set_xlabel(\"Word\",fontsize=ax_size)\n",
    "# ax.set_ylabel(\"Word\",fontsize=ax_size)\n",
    "ax.set_title(f\"Word Cosine-Similarity\",fontsize=title_size) \n",
    "plt.savefig(f\"wordSim.png\", dpi=400, bbox_inches=\"tight\") # TODO: repeat this for the original training set case too!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e3847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
    "fig.suptitle('Word Embedding Dimensionality Reduction', fontsize=title_size + 3)\n",
    "\n",
    "corpus_to_plot = mini_corpus # TODO: change to be the IMDB corpus!!!!\n",
    "\n",
    "wv = model.wv\n",
    "word_ids = list(range(len(wv)))\n",
    "X_words = wv[word_ids]\n",
    "\n",
    "ax = axes[0]\n",
    "\n",
    "pca_words = PCA(n_components=2)\n",
    "X_pca_words = pca_words.fit_transform(X_words)\n",
    "# plot positive and negative separately\n",
    "# ax.scatter(X_pca_words[:neg_ids, 0], X_pca_words[:neg_ids, 1],label=\"POS\")\n",
    "# ax.scatter(X_pca_words[neg_ids:, 0], X_pca_words[neg_ids:, 1],label=\"NEG\")\n",
    "ax.scatter(X_pca_words[:, 0], X_pca_words[:, 1])\n",
    "ax.set_xlabel(\"PC 1\",fontsize=ax_size)\n",
    "ax.set_ylabel(\"PC 2\",fontsize=ax_size)\n",
    "ax.set_title(f\"PCA\",fontsize=title_size) \n",
    "\n",
    "\n",
    "ax = axes[1]\n",
    "\n",
    "tsne_words = TSNE(n_components=2)\n",
    "X_tsne_words = tsne_words.fit_transform(X_words)\n",
    "# plot positive and negative separately\n",
    "ax.scatter(X_tsne_words[:, 0], X_tsne_words[:, 1])\n",
    "ax.set_xlabel(\"Dim 1\",fontsize=ax_size)\n",
    "ax.set_ylabel(\"Dim 2\",fontsize=ax_size)\n",
    "ax.set_title(f\"t-SNE\",fontsize=title_size) \n",
    "\n",
    "plt.savefig(f\"dimRedIMDB_words.png\", dpi=400, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce805b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sorted_docs = sorted([(idx, doc) for idx, (_, doc) in enumerate(mini_corpus.train)], key=lambda x: len(x[1]), reverse=False)\n",
    "idx = 1\n",
    "\n",
    "print(\"Doc length: \", len(sorted_docs[idx][1]))\n",
    "doc_id = sorted_docs[idx][0]\n",
    "doc = ' '.join(sorted_docs[idx][1])\n",
    "print(\"Sample doc: \", doc)\n",
    "\n",
    "dv.most_similar(reviews[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff228db",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.evaluate_word_analogies([\"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743606f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[' '.join(review) for review in reviews[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15afc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
