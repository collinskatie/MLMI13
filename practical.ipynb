{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os, codecs, sys\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# import numpy as np\n",
    "\n",
    "# data_dir = f\"data/aclImdb/\"\n",
    "# sent_dir = f\"{data_dir}test/pos/\"\n",
    "# all_reviews = [rev for rev in os.listdir(sent_dir) if rev[-4:] == \".txt\"]\n",
    "# all_reviews\n",
    "# fpth = f\"{sent_dir}{all_reviews[0]}\"\n",
    "# with open(fpth) as f:\n",
    "#     # NOTE: assumes we want to split by space -- discuss!!\n",
    "#     # e.g., \"kick the bucket\" diff meaning than individ words, but sep by spaces\n",
    "#     full_review_data = f.readlines()[0].split(\" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_review_data[0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kcollins/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import os, codecs, sys\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "data_dir = f\"data/aclImdb/\"\n",
    "sent_dir = f\"{data_dir}test/pos/\"\n",
    "all_reviews = [rev for rev in os.listdir(sent_dir) if rev[-4:] == \".txt\"]\n",
    "all_reviews\n",
    "fpth = f\"{sent_dir}{all_reviews[0]}\"\n",
    "with open(fpth) as f:\n",
    "    # NOTE: assumes we want to split by space -- discuss!!\n",
    "    # e.g., \"kick the bucket\" diff meaning than individ words, but sep by spaces\n",
    "    full_review_data = f.readlines()[0].split(\" \") \n",
    "    \n",
    "# data = [\"I love machine learning. Its awesome.\",\n",
    "#         \"I love coding in python\",\n",
    "#         \"I love building chatbots\",\n",
    "#         \"they chat amagingly well\"]\n",
    "\n",
    "tagged_data2 = [TaggedDocument(doc, [i]) for i, doc in enumerate([full_review_data])]\n",
    "\n",
    "data = [\" \".join(full_review_data)]\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00858445, -0.0025208 ,  0.00340851,  0.00559627, -0.00018348,\n",
       "       -0.00509644, -0.00727983,  0.00172328, -0.00224842,  0.00911236,\n",
       "        0.00550228,  0.00917563, -0.00229089, -0.00023794, -0.00128236,\n",
       "       -0.00012777, -0.00349216,  0.0084849 , -0.00831176, -0.00116774,\n",
       "        0.00279094, -0.00747453, -0.00042225, -0.00442204, -0.00958006,\n",
       "       -0.00708115,  0.00272685,  0.0046035 , -0.00072147,  0.00564601,\n",
       "       -0.00484888,  0.00538824, -0.0041378 , -0.00590575, -0.00047232,\n",
       "        0.00757254,  0.00792406,  0.00834035,  0.00814834,  0.00751941,\n",
       "       -0.00126159, -0.00535745,  0.00380593,  0.00660281,  0.00458905,\n",
       "       -0.00441384, -0.00312149, -0.00883249,  0.00029314,  0.00530691],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "model = Doc2Vec(vector_size=50, window=2)\n",
    "model.build_vocab(tagged_data2)\n",
    "model.train(tagged_data2, total_examples=model.corpus_count, epochs=2)\n",
    "model.infer_vector(full_review_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['POS',\n",
       "  ['based',\n",
       "   'on',\n",
       "   'an',\n",
       "   'actual',\n",
       "   'story,',\n",
       "   'john',\n",
       "   'boorman',\n",
       "   'shows',\n",
       "   'the',\n",
       "   'struggle',\n",
       "   'of',\n",
       "   'an',\n",
       "   'american',\n",
       "   'doctor,',\n",
       "   'whose',\n",
       "   'husband',\n",
       "   'and',\n",
       "   'son',\n",
       "   'were',\n",
       "   'murdered',\n",
       "   'and',\n",
       "   'she',\n",
       "   'was',\n",
       "   'continually',\n",
       "   'plagued',\n",
       "   'with',\n",
       "   'her',\n",
       "   'loss.',\n",
       "   'a',\n",
       "   'holiday',\n",
       "   'to',\n",
       "   'burma',\n",
       "   'with',\n",
       "   'her',\n",
       "   'sister',\n",
       "   'seemed',\n",
       "   'like',\n",
       "   'a',\n",
       "   'good',\n",
       "   'idea',\n",
       "   'to',\n",
       "   'get',\n",
       "   'away',\n",
       "   'from',\n",
       "   'it',\n",
       "   'all,',\n",
       "   'but',\n",
       "   'when',\n",
       "   'her',\n",
       "   'passport',\n",
       "   'was',\n",
       "   'stolen',\n",
       "   'in',\n",
       "   'rangoon,',\n",
       "   'she',\n",
       "   'could',\n",
       "   'not',\n",
       "   'leave',\n",
       "   'the',\n",
       "   'country',\n",
       "   'with',\n",
       "   'her',\n",
       "   'sister,',\n",
       "   'and',\n",
       "   'was',\n",
       "   'forced',\n",
       "   'to',\n",
       "   'stay',\n",
       "   'back',\n",
       "   'until',\n",
       "   'she',\n",
       "   'could',\n",
       "   'get',\n",
       "   'i.d.',\n",
       "   'papers',\n",
       "   'from',\n",
       "   'the',\n",
       "   'american',\n",
       "   'embassy.',\n",
       "   'to',\n",
       "   'fill',\n",
       "   'in',\n",
       "   'a',\n",
       "   'day',\n",
       "   'before',\n",
       "   'she',\n",
       "   'could',\n",
       "   'fly',\n",
       "   'out,',\n",
       "   'she',\n",
       "   'took',\n",
       "   'a',\n",
       "   'trip',\n",
       "   'into',\n",
       "   'the',\n",
       "   'countryside',\n",
       "   'with',\n",
       "   'a',\n",
       "   'tour',\n",
       "   'guide.',\n",
       "   '\"i',\n",
       "   'tried',\n",
       "   'finding',\n",
       "   'something',\n",
       "   'in',\n",
       "   'those',\n",
       "   'stone',\n",
       "   'statues,',\n",
       "   'but',\n",
       "   'nothing',\n",
       "   'stirred',\n",
       "   'in',\n",
       "   'me.',\n",
       "   'i',\n",
       "   'was',\n",
       "   'stone',\n",
       "   'myself.\"',\n",
       "   '<br',\n",
       "   '/><br',\n",
       "   '/>suddenly',\n",
       "   'all',\n",
       "   'hell',\n",
       "   'broke',\n",
       "   'loose',\n",
       "   'and',\n",
       "   'she',\n",
       "   'was',\n",
       "   'caught',\n",
       "   'in',\n",
       "   'a',\n",
       "   'political',\n",
       "   'revolt.',\n",
       "   'just',\n",
       "   'when',\n",
       "   'it',\n",
       "   'looked',\n",
       "   'like',\n",
       "   'she',\n",
       "   'had',\n",
       "   'escaped',\n",
       "   'and',\n",
       "   'safely',\n",
       "   'boarded',\n",
       "   'a',\n",
       "   'train,',\n",
       "   'she',\n",
       "   'saw',\n",
       "   'her',\n",
       "   'tour',\n",
       "   'guide',\n",
       "   'get',\n",
       "   'beaten',\n",
       "   'and',\n",
       "   'shot.',\n",
       "   'in',\n",
       "   'a',\n",
       "   'split',\n",
       "   'second',\n",
       "   'she',\n",
       "   'decided',\n",
       "   'to',\n",
       "   'jump',\n",
       "   'from',\n",
       "   'the',\n",
       "   'moving',\n",
       "   'train',\n",
       "   'and',\n",
       "   'try',\n",
       "   'to',\n",
       "   'rescue',\n",
       "   'him,',\n",
       "   'with',\n",
       "   'no',\n",
       "   'thought',\n",
       "   'of',\n",
       "   'herself.',\n",
       "   'continually',\n",
       "   'her',\n",
       "   'life',\n",
       "   'was',\n",
       "   'in',\n",
       "   'danger.',\n",
       "   '<br',\n",
       "   '/><br',\n",
       "   '/>here',\n",
       "   'is',\n",
       "   'a',\n",
       "   'woman',\n",
       "   'who',\n",
       "   'demonstrated',\n",
       "   'spontaneous,',\n",
       "   'selfless',\n",
       "   'charity,',\n",
       "   'risking',\n",
       "   'her',\n",
       "   'life',\n",
       "   'to',\n",
       "   'save',\n",
       "   'another.',\n",
       "   'patricia',\n",
       "   'arquette',\n",
       "   'is',\n",
       "   'beautiful,',\n",
       "   'and',\n",
       "   'not',\n",
       "   'just',\n",
       "   'to',\n",
       "   'look',\n",
       "   'at;',\n",
       "   'she',\n",
       "   'has',\n",
       "   'a',\n",
       "   'beautiful',\n",
       "   'heart.',\n",
       "   'this',\n",
       "   'is',\n",
       "   'an',\n",
       "   'unforgettable',\n",
       "   'story.',\n",
       "   '<br',\n",
       "   '/><br',\n",
       "   '/>\"we',\n",
       "   'are',\n",
       "   'taught',\n",
       "   'that',\n",
       "   'suffering',\n",
       "   'is',\n",
       "   'the',\n",
       "   'one',\n",
       "   'promise',\n",
       "   'that',\n",
       "   'life',\n",
       "   'always',\n",
       "   'keeps.\"']],\n",
       " ['POS',\n",
       "  ['this',\n",
       "   'is',\n",
       "   'a',\n",
       "   'gem.',\n",
       "   'as',\n",
       "   'a',\n",
       "   'film',\n",
       "   'four',\n",
       "   'production',\n",
       "   '-',\n",
       "   'the',\n",
       "   'anticipated',\n",
       "   'quality',\n",
       "   'was',\n",
       "   'indeed',\n",
       "   'delivered.',\n",
       "   'shot',\n",
       "   'with',\n",
       "   'great',\n",
       "   'style',\n",
       "   'that',\n",
       "   'reminded',\n",
       "   'me',\n",
       "   'some',\n",
       "   'errol',\n",
       "   'morris',\n",
       "   'films,',\n",
       "   'well',\n",
       "   'arranged',\n",
       "   'and',\n",
       "   'simply',\n",
       "   'gripping.',\n",
       "   \"it's\",\n",
       "   'long',\n",
       "   'yet',\n",
       "   'horrifying',\n",
       "   'to',\n",
       "   'the',\n",
       "   'point',\n",
       "   \"it's\",\n",
       "   'excruciating.',\n",
       "   'we',\n",
       "   'know',\n",
       "   'something',\n",
       "   'bad',\n",
       "   'happened',\n",
       "   '(one',\n",
       "   'can',\n",
       "   'guess',\n",
       "   'by',\n",
       "   'the',\n",
       "   'lack',\n",
       "   'of',\n",
       "   'participation',\n",
       "   'of',\n",
       "   'a',\n",
       "   'person',\n",
       "   'in',\n",
       "   'the',\n",
       "   'interviews)',\n",
       "   'but',\n",
       "   'we',\n",
       "   'are',\n",
       "   'compelled',\n",
       "   'to',\n",
       "   'see',\n",
       "   'it,',\n",
       "   'a',\n",
       "   'bit',\n",
       "   'like',\n",
       "   'a',\n",
       "   'car',\n",
       "   'accident',\n",
       "   'in',\n",
       "   'slow',\n",
       "   'motion.',\n",
       "   'the',\n",
       "   'story',\n",
       "   'spans',\n",
       "   'most',\n",
       "   'conceivable',\n",
       "   'aspects',\n",
       "   'and',\n",
       "   'unlike',\n",
       "   'some',\n",
       "   'documentaries',\n",
       "   'did',\n",
       "   'not',\n",
       "   'try',\n",
       "   'and',\n",
       "   'refrain',\n",
       "   'from',\n",
       "   'showing',\n",
       "   'the',\n",
       "   'grimmer',\n",
       "   'sides',\n",
       "   'of',\n",
       "   'the',\n",
       "   'stories,',\n",
       "   'as',\n",
       "   'also',\n",
       "   'dealing',\n",
       "   'with',\n",
       "   'the',\n",
       "   'guilt',\n",
       "   'of',\n",
       "   'the',\n",
       "   'people',\n",
       "   'don',\n",
       "   'left',\n",
       "   'behind',\n",
       "   'him,',\n",
       "   'wondering',\n",
       "   'why',\n",
       "   'they',\n",
       "   \"didn't\",\n",
       "   'stop',\n",
       "   'him',\n",
       "   'in',\n",
       "   'time.',\n",
       "   'it',\n",
       "   'took',\n",
       "   'me',\n",
       "   'a',\n",
       "   'few',\n",
       "   'hours',\n",
       "   'to',\n",
       "   'get',\n",
       "   'out',\n",
       "   'of',\n",
       "   'the',\n",
       "   'melancholy',\n",
       "   'that',\n",
       "   'gripped',\n",
       "   'me',\n",
       "   'after',\n",
       "   'seeing',\n",
       "   'this',\n",
       "   'very-well',\n",
       "   'made',\n",
       "   'documentary.']],\n",
       " ['POS',\n",
       "  ['i',\n",
       "   'really',\n",
       "   'like',\n",
       "   'this',\n",
       "   'show.',\n",
       "   'it',\n",
       "   'has',\n",
       "   'drama,',\n",
       "   'romance,',\n",
       "   'and',\n",
       "   'comedy',\n",
       "   'all',\n",
       "   'rolled',\n",
       "   'into',\n",
       "   'one.',\n",
       "   'i',\n",
       "   'am',\n",
       "   '28',\n",
       "   'and',\n",
       "   'i',\n",
       "   'am',\n",
       "   'a',\n",
       "   'married',\n",
       "   'mother,',\n",
       "   'so',\n",
       "   'i',\n",
       "   'can',\n",
       "   'identify',\n",
       "   'both',\n",
       "   'with',\n",
       "   \"lorelei's\",\n",
       "   'and',\n",
       "   \"rory's\",\n",
       "   'experiences',\n",
       "   'in',\n",
       "   'the',\n",
       "   'show.',\n",
       "   'i',\n",
       "   'have',\n",
       "   'been',\n",
       "   'watching',\n",
       "   'mostly',\n",
       "   'the',\n",
       "   'repeats',\n",
       "   'on',\n",
       "   'the',\n",
       "   'family',\n",
       "   'channel',\n",
       "   'lately,',\n",
       "   'so',\n",
       "   'i',\n",
       "   'am',\n",
       "   'not',\n",
       "   'up-to-date',\n",
       "   'on',\n",
       "   'what',\n",
       "   'is',\n",
       "   'going',\n",
       "   'on',\n",
       "   'now.',\n",
       "   'i',\n",
       "   'think',\n",
       "   'females',\n",
       "   'would',\n",
       "   'like',\n",
       "   'this',\n",
       "   'show',\n",
       "   'more',\n",
       "   'than',\n",
       "   'males,',\n",
       "   'but',\n",
       "   'i',\n",
       "   'know',\n",
       "   'some',\n",
       "   'men',\n",
       "   'out',\n",
       "   'there',\n",
       "   'would',\n",
       "   'enjoy',\n",
       "   'it!',\n",
       "   'i',\n",
       "   'really',\n",
       "   'like',\n",
       "   'that',\n",
       "   'is',\n",
       "   'an',\n",
       "   'hour',\n",
       "   'long',\n",
       "   'and',\n",
       "   'not',\n",
       "   'a',\n",
       "   'half',\n",
       "   'hour,',\n",
       "   'as',\n",
       "   'th',\n",
       "   'hour',\n",
       "   'seems',\n",
       "   'to',\n",
       "   'fly',\n",
       "   'by',\n",
       "   'when',\n",
       "   'i',\n",
       "   'am',\n",
       "   'watching',\n",
       "   'it!',\n",
       "   'give',\n",
       "   'it',\n",
       "   'a',\n",
       "   'chance',\n",
       "   'if',\n",
       "   'you',\n",
       "   'have',\n",
       "   'never',\n",
       "   'seen',\n",
       "   'the',\n",
       "   'show!',\n",
       "   'i',\n",
       "   'think',\n",
       "   'lorelei',\n",
       "   'and',\n",
       "   'luke',\n",
       "   'are',\n",
       "   'my',\n",
       "   'favorite',\n",
       "   'characters',\n",
       "   'on',\n",
       "   'the',\n",
       "   'show',\n",
       "   'though,',\n",
       "   'mainly',\n",
       "   'because',\n",
       "   'of',\n",
       "   'the',\n",
       "   'way',\n",
       "   'they',\n",
       "   'are',\n",
       "   'with',\n",
       "   'one',\n",
       "   'another.',\n",
       "   'how',\n",
       "   'could',\n",
       "   'you',\n",
       "   'not',\n",
       "   'see',\n",
       "   'something',\n",
       "   'was',\n",
       "   'there',\n",
       "   '(or',\n",
       "   'take',\n",
       "   'that',\n",
       "   'long',\n",
       "   'to',\n",
       "   'see',\n",
       "   'it',\n",
       "   'i',\n",
       "   'guess',\n",
       "   'i',\n",
       "   'should',\n",
       "   'say)?',\n",
       "   '<br',\n",
       "   '/><br',\n",
       "   '/>happy',\n",
       "   'viewing!']],\n",
       " ['POS',\n",
       "  ['this',\n",
       "   'is',\n",
       "   'the',\n",
       "   'best',\n",
       "   '3-d',\n",
       "   'experience',\n",
       "   'disney',\n",
       "   'has',\n",
       "   'at',\n",
       "   'their',\n",
       "   'themeparks.',\n",
       "   'this',\n",
       "   'is',\n",
       "   'certainly',\n",
       "   'better',\n",
       "   'than',\n",
       "   'their',\n",
       "   'original',\n",
       "   \"1960's\",\n",
       "   'acid-trip',\n",
       "   'film',\n",
       "   'that',\n",
       "   'was',\n",
       "   'in',\n",
       "   \"it's\",\n",
       "   'place,',\n",
       "   'is',\n",
       "   'leagues',\n",
       "   'better',\n",
       "   'than',\n",
       "   '\"honey',\n",
       "   'i',\n",
       "   'shrunk',\n",
       "   'the',\n",
       "   'audience\"',\n",
       "   '(and',\n",
       "   'far',\n",
       "   'more',\n",
       "   'fun),',\n",
       "   'barely',\n",
       "   'squeaks',\n",
       "   'by',\n",
       "   'the',\n",
       "   'muppetvision',\n",
       "   '3-d',\n",
       "   'movie',\n",
       "   'at',\n",
       "   'disney-mgm',\n",
       "   'and',\n",
       "   'can',\n",
       "   'even',\n",
       "   'beat',\n",
       "   'the',\n",
       "   'original',\n",
       "   '3-d',\n",
       "   '\"movie',\n",
       "   'experience\"',\n",
       "   'captain',\n",
       "   'eo.',\n",
       "   'this',\n",
       "   'film',\n",
       "   'relives',\n",
       "   'some',\n",
       "   'of',\n",
       "   \"disney's\",\n",
       "   'greatest',\n",
       "   'musical',\n",
       "   'hits',\n",
       "   'from',\n",
       "   'aladdin,',\n",
       "   'the',\n",
       "   'little',\n",
       "   'mermaid,',\n",
       "   'and',\n",
       "   'others,',\n",
       "   'and',\n",
       "   'brought',\n",
       "   'a',\n",
       "   'smile',\n",
       "   'to',\n",
       "   'my',\n",
       "   'face',\n",
       "   'throughout',\n",
       "   'the',\n",
       "   'entire',\n",
       "   'show.',\n",
       "   'this',\n",
       "   'is',\n",
       "   'a',\n",
       "   'totally',\n",
       "   'kid-friendly',\n",
       "   'movie',\n",
       "   'too,',\n",
       "   'unlike',\n",
       "   '\"honey...\"',\n",
       "   'and',\n",
       "   'has',\n",
       "   'more',\n",
       "   'effects',\n",
       "   'than',\n",
       "   'the',\n",
       "   'spectacular',\n",
       "   '\"muppetvision\"']],\n",
       " ['POS',\n",
       "  ['of',\n",
       "   'the',\n",
       "   'korean',\n",
       "   'movies',\n",
       "   \"i've\",\n",
       "   'seen,',\n",
       "   'only',\n",
       "   'three',\n",
       "   'had',\n",
       "   'really',\n",
       "   'stuck',\n",
       "   'with',\n",
       "   'me.',\n",
       "   'the',\n",
       "   'first',\n",
       "   'is',\n",
       "   'the',\n",
       "   'excellent',\n",
       "   'horror',\n",
       "   'a',\n",
       "   'tale',\n",
       "   'of',\n",
       "   'two',\n",
       "   'sisters.',\n",
       "   'the',\n",
       "   'second',\n",
       "   'and',\n",
       "   'third',\n",
       "   '-',\n",
       "   'and',\n",
       "   'now',\n",
       "   'fourth',\n",
       "   'too',\n",
       "   '-',\n",
       "   'have',\n",
       "   'all',\n",
       "   'been',\n",
       "   'park',\n",
       "   'chan',\n",
       "   \"wook's\",\n",
       "   'movies,',\n",
       "   'namely',\n",
       "   'oldboy,',\n",
       "   'sympathy',\n",
       "   'for',\n",
       "   'lady',\n",
       "   'vengeance),',\n",
       "   'and',\n",
       "   'now',\n",
       "   'thirst.',\n",
       "   '<br',\n",
       "   '/><br',\n",
       "   '/>park',\n",
       "   'kinda',\n",
       "   'reminds',\n",
       "   'me',\n",
       "   'of',\n",
       "   'quentin',\n",
       "   'tarantino',\n",
       "   'with',\n",
       "   'his',\n",
       "   'irreverence',\n",
       "   'towards',\n",
       "   'convention.',\n",
       "   'all',\n",
       "   'his',\n",
       "   'movies',\n",
       "   'are',\n",
       "   'shocking,',\n",
       "   'but',\n",
       "   'not',\n",
       "   'in',\n",
       "   'a',\n",
       "   'gratuitous',\n",
       "   'sense.',\n",
       "   \"it's\",\n",
       "   'more',\n",
       "   'like',\n",
       "   'he',\n",
       "   'shows',\n",
       "   'us',\n",
       "   'what',\n",
       "   'we',\n",
       "   \"don't\",\n",
       "   'expect',\n",
       "   'to',\n",
       "   'see',\n",
       "   '-',\n",
       "   'typically',\n",
       "   'situations',\n",
       "   'that',\n",
       "   'go',\n",
       "   'radically',\n",
       "   'against',\n",
       "   \"society's\",\n",
       "   'morals,',\n",
       "   'like',\n",
       "   'incest',\n",
       "   'or',\n",
       "   'a',\n",
       "   'libidinous,',\n",
       "   'blood-sucking,',\n",
       "   'yet',\n",
       "   'devout',\n",
       "   'priest.',\n",
       "   \"he's\",\n",
       "   'also',\n",
       "   'quite',\n",
       "   'artistically-inclined',\n",
       "   'with',\n",
       "   'regards',\n",
       "   'to',\n",
       "   'cinematography,',\n",
       "   'and',\n",
       "   'his',\n",
       "   'movies',\n",
       "   'are',\n",
       "   'among',\n",
       "   'the',\n",
       "   'more',\n",
       "   'gorgeous',\n",
       "   'that',\n",
       "   \"i've\",\n",
       "   'seen.<br',\n",
       "   '/><br',\n",
       "   '/>thirst',\n",
       "   'is',\n",
       "   'all',\n",
       "   'that',\n",
       "   '-',\n",
       "   'being',\n",
       "   'about',\n",
       "   'said',\n",
       "   'priest',\n",
       "   'and',\n",
       "   'the',\n",
       "   'repressed,',\n",
       "   'conscience-less',\n",
       "   'woman',\n",
       "   'he',\n",
       "   'falls',\n",
       "   'for',\n",
       "   '-',\n",
       "   'and',\n",
       "   'more.',\n",
       "   \"it's\",\n",
       "   'horror,',\n",
       "   'drama,',\n",
       "   'and',\n",
       "   'even',\n",
       "   'comedy,',\n",
       "   'as',\n",
       "   'park',\n",
       "   'disarms',\n",
       "   'his',\n",
       "   'audience',\n",
       "   'with',\n",
       "   'many',\n",
       "   'inappropriate',\n",
       "   'yet',\n",
       "   'humorous',\n",
       "   'situations.',\n",
       "   'as',\n",
       "   'such,',\n",
       "   'this',\n",
       "   'might',\n",
       "   'be',\n",
       "   'his',\n",
       "   'best',\n",
       "   'work',\n",
       "   'for',\n",
       "   'me',\n",
       "   'yet,',\n",
       "   'since',\n",
       "   'his',\n",
       "   'other',\n",
       "   'two',\n",
       "   'movies',\n",
       "   'that',\n",
       "   \"i've\",\n",
       "   'seen',\n",
       "   'were',\n",
       "   'lacking',\n",
       "   'the',\n",
       "   'humor',\n",
       "   'element',\n",
       "   'that',\n",
       "   \"would've\",\n",
       "   'made',\n",
       "   'them',\n",
       "   'more',\n",
       "   'palatable',\n",
       "   'for',\n",
       "   'repeat',\n",
       "   'viewings.']]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train: 1800, num test: 200\n",
      "tot num reviews: 2000\n",
      "--- classifying reviews using sentiment lexicon  ---\n",
      "--- using document embeddings ---\n",
      "num train: 25000, num test: 25000\n",
      "tot num reviews: 50000\n"
     ]
    }
   ],
   "source": [
    "from Corpora import MovieReviewCorpus\n",
    "from Lexicon import SentimentLexicon\n",
    "from Statistics import SignTest\n",
    "from Classifiers import NaiveBayesText, SVMText\n",
    "from Extensions import SVMDoc2Vec, DocFeaturizer\n",
    "\n",
    "\n",
    "# retrieve corpus\n",
    "corpus=MovieReviewCorpus(stemming=False,pos=False)\n",
    "\n",
    "# use sign test for all significance testing\n",
    "signTest=SignTest()\n",
    "\n",
    "print(\"--- classifying reviews using sentiment lexicon  ---\")\n",
    "\n",
    "# read in lexicon\n",
    "lexicon=SentimentLexicon()\n",
    "\n",
    "# on average there are more positive than negative words per review (~7.13 more positive than negative per review)\n",
    "# to take this bias into account will use threshold (roughly the bias itself) to make it harder to classify as positive\n",
    "# todo: vary this!!! \n",
    "threshold=8\n",
    "\n",
    "# question 8.0\n",
    "print(\"--- using document embeddings ---\")\n",
    "# load in IMDB dataset [TODO!!]\n",
    "corpus=MovieReviewCorpus(stemming=False,pos=False,use_imdb=True)\n",
    "\n",
    "# train doc2vec\n",
    "feature_dim = 200\n",
    "window = 5\n",
    "epochs = 100\n",
    "DF = DocFeaturizer(feature_dim, window, dm=0, dbow_words=1, dm_concat=0)\n",
    "DF.train_model(corpus.train, epochs)\n",
    "\n",
    "SVMDV=SVMDoc2Vec(model=DF,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "SVMDV.train(corpus.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMDV=SVMDoc2Vec(model=DF,bigrams=True,trigrams=False,discard_closed_class=False)\n",
    "# SVMDV.train(corpus.train)\n",
    "\n",
    "SVMDV.test(corpus.test)\n",
    "svm_doc2vec_preds=SVMDV.predictions\n",
    "print(f\"Accuracy: {SVMDV.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {SVMDV.getStdDeviation():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/>did',\n",
       " 'extra-marital',\n",
       " '\"oddities\",',\n",
       " 'ralphie',\n",
       " 'katharine',\n",
       " 'work...count',\n",
       " 'recap.',\n",
       " 'forthright,',\n",
       " 'fontanelles',\n",
       " '\"va-poo-rize\"',\n",
       " 'boosh;',\n",
       " 'three!',\n",
       " 'mwah,',\n",
       " 'frontiers.',\n",
       " '/>taji',\n",
       " 'ruta',\n",
       " '999\"',\n",
       " 'period.',\n",
       " 'morn',\n",
       " \"'intervention'\",\n",
       " 'prizefighter.',\n",
       " 'symbol.<br',\n",
       " 'uncertainties',\n",
       " 'fu...',\n",
       " 'capped-off',\n",
       " '\"ugly',\n",
       " 'usage',\n",
       " 'access',\n",
       " 'montford:',\n",
       " 'loves...but',\n",
       " 'meridian\").<br',\n",
       " 'reeds',\n",
       " 'unrelatable.',\n",
       " 'haggard.',\n",
       " 'ruthlessly,',\n",
       " 'emma?!',\n",
       " 'grovelling',\n",
       " 'is--very',\n",
       " \"northam(who's\",\n",
       " 'wtc).',\n",
       " 'release!\"',\n",
       " 'two-scener',\n",
       " 'anti-islam',\n",
       " '*good*',\n",
       " 'voogdt.<br',\n",
       " '/>jillian',\n",
       " 'almonds',\n",
       " 'unintentional',\n",
       " 'laboratory,',\n",
       " 'shrieks,',\n",
       " '\"reappearance\"',\n",
       " 'starling',\n",
       " '(\"versus\",',\n",
       " 'herd.<br',\n",
       " 'go--as',\n",
       " \"scares'\",\n",
       " 'live...but',\n",
       " 'wongo\"',\n",
       " 'for...a',\n",
       " 'homelands,',\n",
       " 'foresay',\n",
       " 'here..',\n",
       " '(pip',\n",
       " \"woulnd't\",\n",
       " 'purse;',\n",
       " 'that\\x85',\n",
       " 'meteors',\n",
       " 'spelunking',\n",
       " 'strobing',\n",
       " '\"fop',\n",
       " 'seethe',\n",
       " 'hi-jinks,',\n",
       " 'imagination;',\n",
       " 'director/writer/producer',\n",
       " '(1983).',\n",
       " 'samir,',\n",
       " 'informants',\n",
       " 'catching,',\n",
       " 'cleans',\n",
       " '\"da',\n",
       " 'weston),',\n",
       " '\"actors\\'\"',\n",
       " '\"sub-culture\"',\n",
       " 'argentine',\n",
       " 'outing,',\n",
       " 'onw',\n",
       " 'upscale,',\n",
       " 'deterrent.',\n",
       " 'development...someone',\n",
       " 'adventures).',\n",
       " 'aguirre,',\n",
       " 'paris...',\n",
       " 'evolution.<br',\n",
       " 'unfortuneatley',\n",
       " \"taboos'\",\n",
       " 'movies....chris....',\n",
       " '\"sophisticated\"',\n",
       " 'clunky.',\n",
       " \"pressuburger's\",\n",
       " 'quitte',\n",
       " 'revisions,',\n",
       " 'kneecap,',\n",
       " 'astro-chemist',\n",
       " 'cars),',\n",
       " 'internationalize',\n",
       " 'temptresses,',\n",
       " 'mountainside.',\n",
       " \"'average',\",\n",
       " 'colors.<br',\n",
       " 'price,',\n",
       " 'post-dubbed',\n",
       " 'wife\".',\n",
       " 'evil).',\n",
       " 'kwok',\n",
       " 'security.<br',\n",
       " 'etc),',\n",
       " 'exposes,',\n",
       " 'edge:\"',\n",
       " 'correlation;',\n",
       " 'sympathy,',\n",
       " \"'carter'\",\n",
       " '(alix',\n",
       " 'mukerjee',\n",
       " 'werewolf.',\n",
       " 'satisfied;',\n",
       " 'informal,',\n",
       " 'ak-47s,',\n",
       " '/>nic',\n",
       " 're-watch-ability',\n",
       " 'uh,',\n",
       " 'schaefer)',\n",
       " '(blown',\n",
       " '(akin',\n",
       " 'dead;',\n",
       " 'vessels.',\n",
       " \"/>'papi,\",\n",
       " 'exist.<br',\n",
       " 'scat',\n",
       " '(farting)',\n",
       " 'springer.',\n",
       " 'him',\n",
       " 'mouse/rat',\n",
       " 'yelnats,',\n",
       " 'fling,',\n",
       " '(holt).',\n",
       " 'kartiff',\n",
       " 'marchand).',\n",
       " 're-dub',\n",
       " 'mm,',\n",
       " 'hardworker',\n",
       " 'stains,',\n",
       " 'recesses',\n",
       " 'son.)',\n",
       " 'pretensions:<br',\n",
       " 'heavens...how',\n",
       " 'demonstrate',\n",
       " '3000,',\n",
       " \"chicago's\",\n",
       " 'tree-huggers',\n",
       " \"'scream\",\n",
       " '(pretty',\n",
       " '\"soundstage',\n",
       " 'miniskirt,',\n",
       " 'learning,',\n",
       " 'improv.',\n",
       " 'holliman',\n",
       " '\"teenage\"',\n",
       " 'spinelessly?',\n",
       " 'plain.',\n",
       " 'twerp',\n",
       " 'muerte...',\n",
       " 'kol',\n",
       " \"'burlesque\",\n",
       " 'bod.<br',\n",
       " 'reaction.',\n",
       " 'simpsons\")',\n",
       " \"'posh\",\n",
       " '<<<<<<<',\n",
       " 'grays',\n",
       " '/>\"hubiriffic\"',\n",
       " 'everbody',\n",
       " 'grey),',\n",
       " 'toys!',\n",
       " 'trash',\n",
       " 'younes,',\n",
       " 'nod.',\n",
       " 'spiritual,',\n",
       " 'ails',\n",
       " 'jack-leg',\n",
       " 'two!!!',\n",
       " 'not-very-bright',\n",
       " 'color),',\n",
       " '/>natali',\n",
       " '\"terry.\"',\n",
       " 'ants\",',\n",
       " 'arrives!',\n",
       " 'delivers.<br',\n",
       " 'strong',\n",
       " \"`lifer'\",\n",
       " 'nina',\n",
       " 'emblemized',\n",
       " 'heights?',\n",
       " 'gama',\n",
       " 'disappointment.but',\n",
       " \"'fuhrer'\",\n",
       " 'dosent',\n",
       " 'differ!',\n",
       " 'clair,',\n",
       " 'space-faring,',\n",
       " 'kibitzed',\n",
       " 'foul!',\n",
       " 'patch',\n",
       " 'giggle',\n",
       " 'citizen-aged',\n",
       " 'rutger)',\n",
       " 'ishtar,',\n",
       " 'intolerant,',\n",
       " 'waterfall,\"',\n",
       " 'qualify.',\n",
       " 'stoppage',\n",
       " 'wrestlers!',\n",
       " '\"docudrama,\"',\n",
       " 'grayson.',\n",
       " 'cecily',\n",
       " 'frankenstein\").<br',\n",
       " 'invulnerability',\n",
       " 'strong.overall,if',\n",
       " 'traction',\n",
       " '\"frat',\n",
       " 'toe-to-toe.',\n",
       " 'royersford,',\n",
       " 'blockbuster..',\n",
       " '/>rain',\n",
       " '\"continuity\"',\n",
       " '\"chinatown\",',\n",
       " '(italian',\n",
       " 'glitzed',\n",
       " 'americans.<br',\n",
       " \"clarence's\",\n",
       " 'ngema',\n",
       " 'maybe--something',\n",
       " 'responds.',\n",
       " '(margot',\n",
       " \"prejudice,'\",\n",
       " 'hinter',\n",
       " 'hell.\"',\n",
       " '(godzilla)for',\n",
       " 'lusted',\n",
       " 'jonesing',\n",
       " 'good....check',\n",
       " 'dyptic',\n",
       " 'dale.<br',\n",
       " 'continual',\n",
       " 'noncomplicated,',\n",
       " 'show!<br',\n",
       " 'robe,',\n",
       " 'miles',\n",
       " 'intellectually-challenged',\n",
       " 'portrays--',\n",
       " 'country!',\n",
       " 'slating',\n",
       " 'yourself.whatever',\n",
       " 'seafaring',\n",
       " 'strangles',\n",
       " '(paxinou)',\n",
       " 'gosha',\n",
       " 'mind-numbing,',\n",
       " 'fixed',\n",
       " 'humor.sassy',\n",
       " 'coiffure',\n",
       " 'junkyard',\n",
       " 'shield\"',\n",
       " 'way\\x85',\n",
       " 'trucker',\n",
       " 'busby',\n",
       " 'bbc1',\n",
       " 'scary),',\n",
       " \"hagar's\",\n",
       " 'friends.\"',\n",
       " 'muffins',\n",
       " 'compilations',\n",
       " '/>pasqual',\n",
       " \"'reserved'\",\n",
       " 'hietala.',\n",
       " '/>rosario',\n",
       " 'reanimated',\n",
       " 're-interpretation',\n",
       " 'ppv<br',\n",
       " 'where)',\n",
       " \"hazlehurst's\",\n",
       " '(3-3)',\n",
       " '\"hacking',\n",
       " 'but,if',\n",
       " 'pew',\n",
       " 'welch',\n",
       " 'star.<br',\n",
       " 'fronts:',\n",
       " 'merchandise...',\n",
       " 'scenery.and',\n",
       " 'invalids',\n",
       " 'silents',\n",
       " 'movie-making.<br',\n",
       " \"see',\",\n",
       " 'un-curable',\n",
       " \"luzhin's\",\n",
       " '/>hickock',\n",
       " \"witch's\",\n",
       " '*watch*',\n",
       " 'karmas',\n",
       " 'ney',\n",
       " 'show]<br',\n",
       " '\"tetsuo',\n",
       " 'consciously',\n",
       " 'fishing',\n",
       " 'keenly-honed',\n",
       " 'becomes?',\n",
       " 'naff.',\n",
       " 'argento',\n",
       " 'hopcraft,',\n",
       " 'furbies',\n",
       " 'disinfectant',\n",
       " 'eric:',\n",
       " 'pilot!',\n",
       " 'vunerablitity',\n",
       " 'waissbluth',\n",
       " 'jack;',\n",
       " 'shiven',\n",
       " 'sheeting',\n",
       " 'you!\"?',\n",
       " 'perceptions,',\n",
       " 'war\":',\n",
       " '\"was',\n",
       " 'nonsense!!<br',\n",
       " 'sexploitation/',\n",
       " 'sharia',\n",
       " 'frogballs',\n",
       " 'della!',\n",
       " 'pool.',\n",
       " 'talented!!.',\n",
       " 'flesheaters',\n",
       " 'yashraj,',\n",
       " 'opera--what',\n",
       " 'polar-opposites',\n",
       " 'minded.',\n",
       " 'kneels',\n",
       " 'raj/jai(abhishek',\n",
       " 'every1',\n",
       " 'benumbed',\n",
       " \"actually'\",\n",
       " 'expressions,',\n",
       " 'watch).<br',\n",
       " 'recreates',\n",
       " 'directs\"',\n",
       " 'counterparts.',\n",
       " \"remake....it's\",\n",
       " \"leith's\",\n",
       " '90,000',\n",
       " 'screams);',\n",
       " 'all-purpose',\n",
       " 'land-based',\n",
       " 'regrets,',\n",
       " 'power.\"',\n",
       " '/>nb.',\n",
       " \"verona's\",\n",
       " 'on].',\n",
       " 'dvr.',\n",
       " \"pertwee's\",\n",
       " 'connected.',\n",
       " 'salvation.',\n",
       " 'clash.',\n",
       " \"simonetti's\",\n",
       " 'santa,a',\n",
       " '/>derived',\n",
       " 'comment...<br',\n",
       " '\"coffin',\n",
       " 'sympathising',\n",
       " 'sisters\"',\n",
       " 'craze;',\n",
       " 'collaborations,',\n",
       " 'chori',\n",
       " \"mayor's\",\n",
       " 'sumptuous,',\n",
       " '(pivotal',\n",
       " 'convincing).<br',\n",
       " '(1824)',\n",
       " 'predictability!',\n",
       " 'remake!',\n",
       " 'one.burt',\n",
       " 'greece!',\n",
       " 'birds--is',\n",
       " '(cameron',\n",
       " 'anywhere..maybe?',\n",
       " 'really????........it´s',\n",
       " 'ever,',\n",
       " 'cregar)',\n",
       " 'enough...',\n",
       " 'meance\"',\n",
       " 'creek;',\n",
       " 'regular.<br',\n",
       " 'soon!',\n",
       " 'control,',\n",
       " 'mid-film',\n",
       " \"'look'\",\n",
       " 'mid-thirties',\n",
       " 'humanized',\n",
       " \"outskirt's\",\n",
       " 'attractive.<br',\n",
       " \"hadn't,\",\n",
       " 'running-time,',\n",
       " 'queenly',\n",
       " 'sulks',\n",
       " 'ending?',\n",
       " 'irritating...but',\n",
       " 'sutherland,',\n",
       " 'two-timer',\n",
       " 'instead!!!',\n",
       " 'cable),',\n",
       " 'simpson.<br',\n",
       " 'still,this',\n",
       " 'perpetuated,',\n",
       " 'nasa.',\n",
       " 'wolfstein',\n",
       " 'corridor\"',\n",
       " 'dench).<br',\n",
       " 'acrobat',\n",
       " 'mockney',\n",
       " 'channel!)',\n",
       " 'sex-hungry',\n",
       " 'lupa',\n",
       " 'dunston\".',\n",
       " 'joke.....first',\n",
       " 'posse.',\n",
       " '\"chris\".',\n",
       " 'accuracy),',\n",
       " 'pheonix).',\n",
       " 'popoff?,',\n",
       " 'harvard-art-major-film-noir-weenie',\n",
       " '/>asner',\n",
       " 'showgirls.<br',\n",
       " 'book!!',\n",
       " '\"documentary,\"',\n",
       " 'validate',\n",
       " 'grape',\n",
       " 'vaughan.',\n",
       " 'anakin,',\n",
       " 'mummy?',\n",
       " 'unconditionally.much',\n",
       " 'dearest.\"<br',\n",
       " 'softening,',\n",
       " 'performances?',\n",
       " 'wilson),',\n",
       " 'kingdom,',\n",
       " 'ills.<br',\n",
       " 'fox',\n",
       " 'probation,',\n",
       " 'kneeling,',\n",
       " 'spotless',\n",
       " 'widgery',\n",
       " 'goldoni,',\n",
       " 'continuously.',\n",
       " 'sulk!\",',\n",
       " 'quite,',\n",
       " '>funny).<br',\n",
       " 'kick-boxing,',\n",
       " \"o'stern's\",\n",
       " 'blackwood.',\n",
       " 'chicken!)',\n",
       " '\"rights',\n",
       " 'stupendous.',\n",
       " 'wet',\n",
       " 'toler,',\n",
       " 'established,',\n",
       " 'ex-husband.',\n",
       " 'frogs),',\n",
       " 'flutes',\n",
       " '/>karen',\n",
       " 'ultimately',\n",
       " \"1/2'\",\n",
       " \"bone's\",\n",
       " 'valentinov,',\n",
       " 'felitta',\n",
       " '1873),',\n",
       " 'petaluma',\n",
       " 'post)',\n",
       " 'violence--all',\n",
       " 'dialogues,but',\n",
       " '2006<br',\n",
       " '/>\"bleak',\n",
       " 'that,any',\n",
       " 'teens.<br',\n",
       " 'relationships--especially',\n",
       " 'priyadarshan/vohra',\n",
       " 'casanova.',\n",
       " 'phew...you',\n",
       " 'charachter',\n",
       " 'dr.mordrid',\n",
       " '\"zombie,\"',\n",
       " 'cheney,',\n",
       " 'mice?',\n",
       " 'silvestres),',\n",
       " 'milestone',\n",
       " 'osterwald',\n",
       " 'machinery,',\n",
       " 'unknowns,',\n",
       " '(setting/who',\n",
       " 'governing',\n",
       " 'noticed?<br',\n",
       " 'voyager!',\n",
       " 'delinquents',\n",
       " 'massude',\n",
       " 'inspiration),',\n",
       " 'soldier-entrepreneur',\n",
       " 'quintessential,',\n",
       " 'themes?',\n",
       " '/>alexander',\n",
       " 'cosgrove-hall',\n",
       " 'x-box',\n",
       " 'pace.<br',\n",
       " 'plot(could',\n",
       " 'him--not',\n",
       " 'dessertion',\n",
       " \"forgettable.don't\",\n",
       " 'onto',\n",
       " 'celeste;',\n",
       " 'buyout',\n",
       " '\"impress\"',\n",
       " 'sir!\"',\n",
       " 's**t,',\n",
       " '(miranda',\n",
       " 'celine.',\n",
       " 'have:<br',\n",
       " 'thousand.',\n",
       " 'dragging.',\n",
       " 'snooze-fest',\n",
       " 'pfink',\n",
       " 'transitions.',\n",
       " '/>-chris',\n",
       " 'doers',\n",
       " 'lost)<br',\n",
       " 'raoul.',\n",
       " 'nordon',\n",
       " 'settling',\n",
       " '/>mitch',\n",
       " '(giacomo',\n",
       " 'ny.',\n",
       " 'rafifi,',\n",
       " 'extenuating',\n",
       " 'biceps.<br',\n",
       " 'hilarious.....sally',\n",
       " 'duration',\n",
       " 'believes,',\n",
       " 'wendigo,',\n",
       " 'sanders),\"date',\n",
       " '(jane)',\n",
       " '\"daniel',\n",
       " 'frames!',\n",
       " 'writer-dude',\n",
       " 'that?<br',\n",
       " \"'house'.\",\n",
       " 'panamanian',\n",
       " 'embers',\n",
       " 'detestable,',\n",
       " '\"lesser\"',\n",
       " 'umrao',\n",
       " 'coward),',\n",
       " 'ideals.<br',\n",
       " 'of-course',\n",
       " 'pictures.<br',\n",
       " 'diddled',\n",
       " 'cooled,',\n",
       " 'resister).',\n",
       " 'pauses.',\n",
       " 'inlay.',\n",
       " 'enjoyable!',\n",
       " 'honest',\n",
       " 'purchased.',\n",
       " 'ending,surprise,there',\n",
       " '-horrible',\n",
       " 'compensations.',\n",
       " 'parents\",',\n",
       " 'loyalist,',\n",
       " 'superman.',\n",
       " 'write.',\n",
       " 'radio.<br',\n",
       " 'smudges)',\n",
       " '/>borzage',\n",
       " 'scar.',\n",
       " 'missile,\"',\n",
       " 'waxwork',\n",
       " 'well-portrayed.',\n",
       " 'unluckiest',\n",
       " 'universe:',\n",
       " '(owen',\n",
       " 'rice-paper',\n",
       " 'desilva',\n",
       " 'again...ugh.',\n",
       " 'cohering',\n",
       " 'badness?',\n",
       " '/>.',\n",
       " 'influenced.',\n",
       " 'japanes',\n",
       " 'spartacus.',\n",
       " \"thurman's\",\n",
       " '/>alicia',\n",
       " 'franciosa)',\n",
       " 'more....<br',\n",
       " '\"lawmen\"',\n",
       " 'passed',\n",
       " \"song',\",\n",
       " 'hrabosky',\n",
       " \"why'd\",\n",
       " 'nuttery',\n",
       " \"'hating\",\n",
       " '/>party',\n",
       " 'laborers.',\n",
       " 'nerdish',\n",
       " 'case,check',\n",
       " 'timberlane',\n",
       " 'shocked?<br',\n",
       " 'like...\"',\n",
       " 'appealing,',\n",
       " 'language,',\n",
       " 'vila',\n",
       " 'me....)...the',\n",
       " 'vetoes',\n",
       " 'attempts),',\n",
       " 'escape...just',\n",
       " 'venom.',\n",
       " '30s.',\n",
       " 'turtle,\"',\n",
       " 'mammonist',\n",
       " 'climax,though',\n",
       " 'cowers,',\n",
       " 'observations:<br',\n",
       " 'afoul',\n",
       " 'figaro.\"<br',\n",
       " 'austen',\n",
       " 'terence',\n",
       " 'prey-predator',\n",
       " 'supersadlysoftie',\n",
       " ',if',\n",
       " 'adores',\n",
       " 'hauntings',\n",
       " 'on-the-edge',\n",
       " 'kaafi',\n",
       " '\"tuileries\").<br',\n",
       " 'ending,unlike',\n",
       " 'living!',\n",
       " 'mcg',\n",
       " 'pervasive',\n",
       " 'hollywood-produced',\n",
       " 'scottish)',\n",
       " 'remember).',\n",
       " 'yawk',\n",
       " '\"sir\".',\n",
       " 'decorative,',\n",
       " 'mile,',\n",
       " 'challiya',\n",
       " 'draine,',\n",
       " 'job/art',\n",
       " '1.7',\n",
       " 'nichol',\n",
       " 'try.<br',\n",
       " 'suspicion.',\n",
       " 'tormented!!!',\n",
       " 'retaliate.<br',\n",
       " 'wacko,',\n",
       " 'mysteriously',\n",
       " 'elements:',\n",
       " 'stevenson:',\n",
       " 'apologists',\n",
       " 'kidding),',\n",
       " '\"evil,\"',\n",
       " 'me--i',\n",
       " 'cassandra',\n",
       " 'winter,',\n",
       " 'remembered,',\n",
       " 'though...',\n",
       " 'dudes',\n",
       " 'out-of-place.',\n",
       " \"out...it's\",\n",
       " 'twenty-year',\n",
       " 'stash',\n",
       " 'foresees',\n",
       " 'baiting,',\n",
       " 'manfred',\n",
       " 'mind-mucks',\n",
       " 'approves',\n",
       " 'english-dubbed',\n",
       " 'pie-fight',\n",
       " 'rotheroe',\n",
       " 'pan...',\n",
       " '6th-century',\n",
       " 'opinion....no',\n",
       " '/>good-night,',\n",
       " 'details--it',\n",
       " 'prodigy-directors',\n",
       " 'love:',\n",
       " 'dumb.<br',\n",
       " 'yokels',\n",
       " \"alan's\",\n",
       " 'zedora',\n",
       " 'diviner',\n",
       " 'pre-fame',\n",
       " 'f-84f',\n",
       " 'kgb/red',\n",
       " 'gravedancers,tooth',\n",
       " 'threats',\n",
       " 'mad,\"',\n",
       " 'frazee',\n",
       " 'highschool,',\n",
       " 'convenient)....<br',\n",
       " 'burnford.kevin',\n",
       " 'enchelada',\n",
       " 'gottlieb',\n",
       " 'store,steal',\n",
       " 'neorealism,',\n",
       " 'shayne)',\n",
       " 'summersisle:',\n",
       " 'damned.)',\n",
       " '(\"saturday',\n",
       " 'ma-china\")',\n",
       " 'jacinto',\n",
       " 'argo',\n",
       " 'turning,',\n",
       " 'best-sustained',\n",
       " 'race-war,',\n",
       " 'super-babe',\n",
       " 'keays',\n",
       " 'science-fiction!',\n",
       " 'ford',\n",
       " 'underwood',\n",
       " 'maggots?',\n",
       " '/>cage',\n",
       " 'by\\x97ambition.\"',\n",
       " '\"poo\"',\n",
       " 'barney.<br',\n",
       " '\"astonishing\"',\n",
       " 'children-miscellaneous',\n",
       " \"/>'should\",\n",
       " \"'terrible\",\n",
       " 'pooch).',\n",
       " 'river,far',\n",
       " 'jarvis',\n",
       " '(isla',\n",
       " 'qian',\n",
       " 'clicked.<br',\n",
       " 'dexterous',\n",
       " \"king'\",\n",
       " 'thomilson',\n",
       " 'family\")',\n",
       " \"barton's\",\n",
       " 'turnstiles,',\n",
       " 'emphasized,',\n",
       " 'cinemas.<br',\n",
       " 'gentile',\n",
       " 'provocative,',\n",
       " \"`super'\",\n",
       " 'central.',\n",
       " '\"alienator\"?',\n",
       " 'skull.\"<br',\n",
       " 'boniface).',\n",
       " 'fantasies',\n",
       " 'over;',\n",
       " 'sung.',\n",
       " 'interaction.',\n",
       " '/>\"sweets',\n",
       " 'phiiistine',\n",
       " '\"taboo\".',\n",
       " 'hems',\n",
       " 'capper.',\n",
       " \"tilly'\",\n",
       " 'twill',\n",
       " '(denholm',\n",
       " 'pantaloons',\n",
       " 'setting.',\n",
       " 'manero',\n",
       " 'transformation,',\n",
       " 'editor)',\n",
       " 'bad-tempered',\n",
       " 'eagle-',\n",
       " 'catch-22,',\n",
       " 'died?<br',\n",
       " 'button!,',\n",
       " 'leno,',\n",
       " 'houseboat',\n",
       " 'cottage\"',\n",
       " 'vibrators',\n",
       " 'work\\x85',\n",
       " 'whoop,',\n",
       " 'sultry,',\n",
       " 'thumbing-of-the-nose',\n",
       " 'home!also',\n",
       " '/>5.',\n",
       " 'sistematski',\n",
       " 'apophis.',\n",
       " '20/20',\n",
       " 'doe',\n",
       " 'better-cast',\n",
       " 'any-movie',\n",
       " 'hammill',\n",
       " 'presses',\n",
       " 'guerilla',\n",
       " 'wrongly,',\n",
       " 'generator,',\n",
       " 'ego..',\n",
       " 'sophistication.',\n",
       " 'interject',\n",
       " \"'realistically'\",\n",
       " \"europe)can't\",\n",
       " 'subsumed',\n",
       " 'dull',\n",
       " \"'enemy'\",\n",
       " 'sofaer-a',\n",
       " 'debate',\n",
       " 'sober.)',\n",
       " 'gore),',\n",
       " 'knife.',\n",
       " 'ting',\n",
       " '(matthew',\n",
       " \"tetsuro's\",\n",
       " '\"examined\"',\n",
       " '/>rob',\n",
       " 'eunuch',\n",
       " 'bazaar.',\n",
       " '(kagemusha).',\n",
       " 'lowly,',\n",
       " 'strangulation.',\n",
       " 'bleakness,',\n",
       " 'coloring,',\n",
       " 'purposes--and',\n",
       " 'benji,and',\n",
       " 'gunmen...<br',\n",
       " 'dumann,',\n",
       " 'excruciating;',\n",
       " 'honduras,',\n",
       " \"'states'.\",\n",
       " 'post-1940',\n",
       " 'mud-wrestling,',\n",
       " \"that?'\",\n",
       " 'one-word',\n",
       " '\"veronica',\n",
       " '\"testament\"',\n",
       " 'occured.',\n",
       " 'one-liner,',\n",
       " '\"daintily',\n",
       " '1984.i',\n",
       " 'coccio(the',\n",
       " 'threads.<br',\n",
       " 'gershwin)',\n",
       " 'no-account',\n",
       " 'bullshit).<br',\n",
       " 'days-',\n",
       " 'film.sure,',\n",
       " 'wanted,',\n",
       " 'fudge!',\n",
       " 'composure',\n",
       " 'dreadcentral.com',\n",
       " 'cantor,',\n",
       " 'passions.<br',\n",
       " 'fêtes\"',\n",
       " \"bucharest's\",\n",
       " 'wrost',\n",
       " 'wrists.<br',\n",
       " 'escort),',\n",
       " 'hepburn--who',\n",
       " '(grant)',\n",
       " \"automobile's\",\n",
       " 'gun-fire,',\n",
       " 're-shoots.<br',\n",
       " 'viewers?',\n",
       " 'campaigns',\n",
       " 'frye,who',\n",
       " 'melville´s',\n",
       " 'honey.<br',\n",
       " 'berlin\"',\n",
       " 'watching?\".<br',\n",
       " 'work-eat-sleep',\n",
       " 'clean,',\n",
       " 'gainax',\n",
       " 'jeunet',\n",
       " 'apartments',\n",
       " 'arrested?',\n",
       " 'tape-recorder',\n",
       " 'wearying,',\n",
       " \"'attack\",\n",
       " 'undertakers',\n",
       " 'p.g.',\n",
       " \"'classics'\",\n",
       " 'attendance',\n",
       " 'them.so',\n",
       " 'verson',\n",
       " 'mocked.',\n",
       " 'insipidness',\n",
       " 'movie*',\n",
       " '/>impartially,',\n",
       " 'army?',\n",
       " 'humvee',\n",
       " 'scam,',\n",
       " 'bliss!<br',\n",
       " 'harold',\n",
       " 'up/spongebob',\n",
       " 'sophistication.<br',\n",
       " 'tightly-wound',\n",
       " '(robert',\n",
       " 'availability.',\n",
       " 'perimeters',\n",
       " 'telling\".',\n",
       " 'monitored.',\n",
       " 'location\",',\n",
       " '/>can´t',\n",
       " 'heading,',\n",
       " \"'redneck\",\n",
       " 'a.)',\n",
       " 'detainee',\n",
       " 'derek,',\n",
       " 'kleinfeld',\n",
       " 'horseshoe.if',\n",
       " 'harp',\n",
       " 'completed.',\n",
       " 'shouting,',\n",
       " 'talley,',\n",
       " \"love'-story,\",\n",
       " 'ballroom',\n",
       " 'outdoor',\n",
       " \"harvest').<br\",\n",
       " \"yonica's\",\n",
       " 'respond.',\n",
       " \"clive's\",\n",
       " 'healthy.<br',\n",
       " 'warbucks,',\n",
       " 'behind.cabin',\n",
       " 'respiratory',\n",
       " \"60's....i'm\",\n",
       " '/>opening',\n",
       " 'american-loathing',\n",
       " \"gamepad'.\",\n",
       " 'carry',\n",
       " 'suffocation,',\n",
       " 'fbi-agent?',\n",
       " \"drool'athon,\",\n",
       " 'locals',\n",
       " 'stating,',\n",
       " 'derivative',\n",
       " 'atley',\n",
       " 'amazing<br',\n",
       " 'stowaway).<br',\n",
       " 'advertising!',\n",
       " 'cameron)',\n",
       " 'school?',\n",
       " 'gaea',\n",
       " 'insulting;',\n",
       " '/poet/',\n",
       " 'medieval?',\n",
       " 'halfassed',\n",
       " 'above-8',\n",
       " 'leonard,',\n",
       " 'wives\"',\n",
       " 'gams',\n",
       " 'clays',\n",
       " \"`catholic',\",\n",
       " 'condescension',\n",
       " 'pasts.',\n",
       " '\"levens',\n",
       " 'either-in',\n",
       " 'outtakes,',\n",
       " 'reassertion',\n",
       " 'advancements',\n",
       " '(www.loveearth.com)<br',\n",
       " 'isobel(elyse',\n",
       " 'grudge,',\n",
       " 'bad?)',\n",
       " 'slr',\n",
       " 'bikes,',\n",
       " 'poisons,',\n",
       " 'buchfellner.',\n",
       " 'relaxed.',\n",
       " 'yukon,',\n",
       " 'camilo',\n",
       " '\"looks\",',\n",
       " 'whim.',\n",
       " 'unanimous',\n",
       " 'techniques.',\n",
       " 'inquires.',\n",
       " '-16s',\n",
       " 'b.meyer.',\n",
       " '(master',\n",
       " 'rose-colored',\n",
       " 'exploitation,',\n",
       " \"belgian's\",\n",
       " 'redirect,',\n",
       " \"x-files'\",\n",
       " \"johanson's\",\n",
       " 'smile...',\n",
       " 'insurance:',\n",
       " 'cut-away',\n",
       " 'wilson,hugh',\n",
       " 'fisted',\n",
       " 'ha!)<br',\n",
       " 'respectably.',\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVMDV.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.model.get_latest_training_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv = DF.model.dv\n",
    "len(dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25694287, -1.0035174 ,  0.43431517, -0.05444425, -0.820195  ,\n",
       "       -0.4199454 ,  0.39094493, -0.08167047, -0.99395066, -0.24792047,\n",
       "        1.0255119 ,  0.15074751, -0.46977296,  0.41093102,  0.09847463,\n",
       "       -0.23182873,  0.35071555, -0.06572556, -0.23514332, -0.6326952 ,\n",
       "        0.03815828,  0.40439993, -0.7058862 ,  0.7861157 , -0.29349357,\n",
       "       -0.8650575 , -0.45422968,  2.0212114 ,  0.20175543, -0.01345561,\n",
       "       -0.74109584, -1.0567218 , -0.4301948 , -0.57295823, -1.5431702 ,\n",
       "       -0.10322809, -0.3301149 ,  0.5573709 , -0.5944951 , -0.3263568 ,\n",
       "        0.00284563, -0.4785149 ,  0.40102598,  0.5861295 , -0.29275712,\n",
       "        0.24139974, -0.69227105,  1.1278989 , -0.4222567 ,  0.3876269 ,\n",
       "       -1.0154034 ,  0.8733377 ,  1.014428  , -0.3919254 ,  0.23799759,\n",
       "       -1.2454207 ,  0.7378977 ,  0.03930977, -0.38281327,  0.0215235 ,\n",
       "        0.19878553, -0.14755936, -0.7931957 , -0.9044487 ,  0.8846119 ,\n",
       "        0.16429038, -0.15522781,  1.3215221 , -0.2716947 ,  0.23194475,\n",
       "        1.0064045 ,  0.8641525 ,  0.1529699 , -0.16670166, -1.1483417 ,\n",
       "       -0.40977177, -0.4276961 , -0.00413383,  0.57280207, -1.4199351 ,\n",
       "       -1.678706  ,  1.1494548 ,  0.6857503 ,  0.25230387,  0.12999317,\n",
       "        0.12630709,  1.2220169 ,  0.00256214, -0.34665793,  0.43462157,\n",
       "       -0.6786038 , -0.20614053,  1.1152716 ,  0.02027079,  0.14509559,\n",
       "        1.894059  ,  0.22011447, -1.1582215 , -0.81250733,  1.4342297 ,\n",
       "        0.02818874, -1.0017642 ,  0.39133018,  0.58239114,  0.91959673,\n",
       "       -0.10717481,  0.8130115 ,  0.2799312 ,  0.09000175, -0.08866424,\n",
       "        0.46817884,  0.6727651 ,  0.92287374,  0.36914393, -0.47533184,\n",
       "        1.3948356 ,  0.15366082,  1.373812  ,  0.65872794,  0.06060508,\n",
       "        0.24454345, -0.99108166,  0.07283565, -1.1687295 , -0.98735785,\n",
       "        0.9351952 , -0.20985712,  0.90367204,  0.53287685,  0.30512586,\n",
       "        0.61081713, -1.2042632 ,  0.30870456, -1.1381499 ,  0.16116141,\n",
       "        0.39252242, -0.62715507,  0.95225763,  0.5663228 ,  0.7234112 ,\n",
       "        0.10911   ,  0.7795559 ,  1.2492199 , -0.7318794 , -0.32015744,\n",
       "       -0.24251144,  0.35990956,  0.6225528 , -0.25518993,  0.2340065 ,\n",
       "       -0.71355224,  0.48355845, -0.13458845, -0.6875931 , -0.69379085,\n",
       "        0.09115624, -0.67575574,  0.15949628, -0.27908975,  0.5727037 ,\n",
       "       -1.3233016 , -0.33982608,  0.11602369,  0.8048829 ,  0.5693564 ,\n",
       "        0.04380366,  0.13033415, -0.12973893,  0.5652064 ,  0.5577735 ,\n",
       "       -0.49561417, -0.15605167,  1.2236245 ,  0.4452724 , -0.6466467 ,\n",
       "       -0.3223264 ,  0.5147454 , -0.11102679,  0.13552374,  0.5740859 ,\n",
       "       -0.7400592 , -0.02947122,  0.24623047, -0.29815352,  0.0295918 ,\n",
       "       -0.5107465 , -0.46838185,  0.69004864, -0.17131972, -1.0863045 ,\n",
       "       -0.31346884, -0.09387012,  0.61344814,  1.5196428 ,  0.40787885,\n",
       "       -0.31988358,  0.16054004, -1.1079949 , -0.41605082,  0.28043175],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv = DF.model.wv\n",
    "wv['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kcollins/MLMI13/Analysis.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_files = np.array([np.array(corpus.folds[fold_j]) for fold_j in range(num_folds) if fold_j != fold_i])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ca4c9c5603f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSVMDV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrossValidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/MLMI13/Analysis.py\u001b[0m in \u001b[0;36mcrossValidate\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold_j\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfold_j\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfold_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLMI13/Classifiers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, reviews)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# function to determine features in training set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;31m# reset SVM classifier and train SVM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLMI13/Extensions.py\u001b[0m in \u001b[0;36mgetFeatures\u001b[0;34m(self, reviews)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mall_review_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractReviewTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# convert between review tokens to doc features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mdoc_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview_tokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_review_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;31m# todo: play with whether we normalize or not!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_vecs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLMI13/Extensions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mall_review_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractReviewTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# convert between review tokens to doc features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mdoc_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview_tokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_review_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;31m# todo: play with whether we normalize or not!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_vecs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLMI13/Extensions.py\u001b[0m in \u001b[0;36minfer_vector\u001b[0;34m(self, doc_tokens)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# note: same name as main class to allow this object to be used w/ other classes w/o modification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36minfer_vector\u001b[0;34m(self, doc_words, alpha, min_alpha, epochs)\u001b[0m\n\u001b[1;32m    647\u001b[0m                 )\n\u001b[1;32m    648\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                 train_document_dm(\n\u001b[0m\u001b[1;32m    650\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneu1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                     \u001b[0mlearn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SVMDV.crossValidate(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = SVMDV.getFeatures(corpus.test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature dim:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kcollins/MLMI13/Analysis.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_files = np.array([np.array(corpus.folds[fold_j]) for fold_j in range(num_folds) if fold_j != fold_i])\n",
      "/Users/kcollins/MLMI13/Analysis.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_files = np.array([np.array(corpus.folds[fold_j]) for fold_j in range(num_folds) if fold_j != fold_i])\n",
      "/Users/kcollins/MLMI13/Analysis.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_files = np.array([np.array(corpus.folds[fold_j]) for fold_j in range(num_folds) if fold_j != fold_i])\n",
      "/Users/kcollins/MLMI13/Analysis.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_files = np.array([np.array(corpus.folds[fold_j]) for fold_j in range(num_folds) if fold_j != fold_i])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3b276979eec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mSVMDV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVMDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrigrams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscard_closed_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mSVMDV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrossValidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msvm_doc2vec_preds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVMDV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy: {SVMDV.getAccuracy():.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLMI13/Analysis.py\u001b[0m in \u001b[0;36mcrossValidate\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold_j\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfold_j\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfold_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLMI13/Classifiers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, reviews)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# function to determine features in training set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;31m# reset SVM classifier and train SVM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLMI13/Extensions.py\u001b[0m in \u001b[0;36mgetFeatures\u001b[0;34m(self, reviews)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mall_review_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractReviewTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# convert between review tokens to doc features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mdoc_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview_tokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_review_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;31m# todo: play with whether we normalize or not!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_vecs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLMI13/Extensions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mall_review_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractReviewTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# convert between review tokens to doc features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mdoc_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview_tokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_review_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;31m# todo: play with whether we normalize or not!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_vecs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLMI13/Extensions.py\u001b[0m in \u001b[0;36minfer_vector\u001b[0;34m(self, doc_tokens)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# note: same name as main class to allow this object to be used w/ other classes w/o modification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36minfer_vector\u001b[0;34m(self, doc_words, alpha, min_alpha, epochs)\u001b[0m\n\u001b[1;32m    647\u001b[0m                 )\n\u001b[1;32m    648\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                 train_document_dm(\n\u001b[0m\u001b[1;32m    650\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneu1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                     \u001b[0mlearn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_dims = [10, 50, 100, 200, 500]\n",
    "window = 5\n",
    "epochs = 100\n",
    "for feature_dim in feature_dims:\n",
    "    print(\"feature dim: \", feature_dim)\n",
    "    DF = DocFeaturizer(feature_dim, window)\n",
    "    DF.train_model(corpus.train, epochs)\n",
    "    SVMDV=SVMDoc2Vec(model=DF,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "    SVMDV.crossValidate(corpus)\n",
    "    svm_doc2vec_preds=SVMDV.predictions\n",
    "    print(f\"Accuracy: {SVMDV.getAccuracy():.2f}\") \n",
    "    print(f\"Std. Dev: {SVMDV.getStdDeviation():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embs = DF.get_embeddings(corpus.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embs = DF.get_embeddings(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__init__() takes exactly one argument (the instance to initialize)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-82f076fa3973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSVM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVMDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrigrams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscard_closed_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLMI13/Extensions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, bigrams, trigrams, discard_closed_class, normalize_vecs)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mrandom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \"\"\"\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVMText\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrigrams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscard_closed_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object.__init__() takes exactly one argument (the instance to initialize)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POS'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply to new prediction problem\n",
    "normalize_vecs = False\n",
    "svm_doc2vec_preds \n",
    "\n",
    "# compare to past models\n",
    "# need to re-train Naive Bayes on this dataset! \n",
    "print(f\"Accuracy: {SVMDV.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {SVMDV.getStdDeviation():.2f}\")\n",
    "p_value=signTest.getSignificance(svm_doc2vec_preds,smoothed_preds)\n",
    "signifance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results for SVM with  {signifance} with respect to Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Corpora import MovieReviewCorpus\n",
    "from Lexicon import SentimentLexicon\n",
    "from Statistics import SignTest\n",
    "from Classifiers import NaiveBayesText, SVMText\n",
    "from Extensions import SVMDoc2Vec, DocFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve corpus\n",
    "corpus=MovieReviewCorpus(stemming=False,pos=False)\n",
    "\n",
    "# use sign test for all significance testing\n",
    "signTest=SignTest()\n",
    "\n",
    "print(\"--- classifying reviews using sentiment lexicon  ---\")\n",
    "\n",
    "# read in lexicon\n",
    "lexicon=SentimentLexicon()\n",
    "\n",
    "# on average there are more positive than negative words per review (~7.13 more positive than negative per review)\n",
    "# to take this bias into account will use threshold (roughly the bias itself) to make it harder to classify as positive\n",
    "# todo: vary this!!! \n",
    "threshold=8\n",
    "\n",
    "# NOTE: from katie to self -- play w/ changing the threshold value! \n",
    "\n",
    "# question 0.1\n",
    "lexicon.classify(corpus.reviews,threshold,magnitude=False)\n",
    "token_preds=lexicon.predictions\n",
    "print(f\"token-only results: {lexicon.getAccuracy():.2f}\")\n",
    "\n",
    "lexicon.classify(corpus.reviews,threshold,magnitude=True)\n",
    "magnitude_preds=lexicon.predictions\n",
    "print(f\"magnitude results: {lexicon.getAccuracy():.2f}\")\n",
    "\n",
    "# question 0.2\n",
    "p_value=signTest.getSignificance(token_preds,magnitude_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"magnitude lexicon results are {significance} with respect to token-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 1.0\n",
    "print(\"--- classifying reviews using Naive Bayes on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus.train)\n",
    "NB.test(corpus.test)\n",
    "# store predictions from classifier\n",
    "non_smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy without smoothing: {NB.getAccuracy():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2.0\n",
    "# use smoothing\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus.train)\n",
    "NB.test(corpus.test)\n",
    "smoothed_preds=NB.predictions\n",
    "# saving this for use later\n",
    "num_non_stemmed_features=len(NB.vocabulary)\n",
    "print(f\"Accuracy using smoothing: {NB.getAccuracy():.2f}\")\n",
    "\n",
    "# question 2.1\n",
    "# see if smoothing significantly improves results\n",
    "p_value=signTest.getSignificance(non_smoothed_preds,smoothed_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing are {significance} with respect to no smoothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3.0\n",
    "print(\"--- classifying reviews using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus)\n",
    "# using cross-eval for smoothed predictions from now on\n",
    "smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.3f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 4.0\n",
    "print(\"--- stemming corpus ---\")\n",
    "# retrieve corpus with tokenized text and stemming (using porter)\n",
    "stemmed_corpus=MovieReviewCorpus(stemming=True,pos=False)\n",
    "print(\"--- cross-validating NB using stemming ---\")\n",
    "NB.crossValidate(stemmed_corpus)\n",
    "stemmed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.3f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.3f}\")\n",
    "\n",
    "# TODO Q4.1\n",
    "# see if stemming significantly improves results on smoothed NB (both did cv)\n",
    "p_value=signTest.getSignificance(stemmed_preds,smoothed_preds) # note compared against version w/ smoothing! \n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using stemming are {significance} with respect to no stemming\")\n",
    "\n",
    "# TODO Q4.2\n",
    "print(\"--- determining the number of features before/after stemming ---\")\n",
    "# (**) changing the number of features == changing the number of words in the vocab \n",
    "NB.train(corpus.train)\n",
    "NB.test(corpus.test)\n",
    "num_stemmed_features = len(NB.vocabulary)\n",
    "print(f\"num features, non-stemmed: {num_non_stemmed_features} vs. num features, stemmed: {num_stemmed_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question Q5.0\n",
    "# cross-validate model using smoothing and bigrams\n",
    "print(\"--- cross-validating naive bayes using smoothing and bigrams ---\")\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=True,trigrams=False,discard_closed_class=False)\n",
    "NB.crossValidate(corpus)\n",
    "smoothed_and_bigram_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.2f}\")\n",
    "\n",
    "\n",
    "# see if bigrams significantly improves results on smoothed NB only\n",
    "p_value=signTest.getSignificance(smoothed_preds,smoothed_and_bigram_preds)\n",
    "signifance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing and bigrams are {signifance} with respect to smoothing only\")\n",
    "\n",
    "\n",
    "# TODO Q5.1\n",
    "# katie: from q3 (num_non_stemmed_features)\n",
    "num_bow_features = len(NB.vocabulary)\n",
    "print(f\"num features for [model] (Q3): {num_non_stemmed_features} vs. num features BoW: {num_bow_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q6 and 6.1\n",
    "print(\"--- classifying reviews using SVM 10-fold cross-eval ---\")\n",
    "SVM=SVMText(bigrams=True,trigrams=False,discard_closed_class=False)\n",
    "SVM.crossValidate(corpus)\n",
    "svm_preds=SVM.predictions\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q7.0\n",
    "print(\"--- adding in POS information to corpus ---\")\n",
    "pos_corpus=MovieReviewCorpus(stemming=False,pos=True)\n",
    "print(\"--- training svm on word+pos features ----\")\n",
    "SVM=SVMText(bigrams=True,trigrams=False,discard_closed_class=False)\n",
    "SVM.crossValidate(pos_corpus)\n",
    "svm_pos_preds=SVM.predictions\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.2f}\")\n",
    "p_value=signTest.getSignificance(svm_pos_preds,svm_preds)\n",
    "signifance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using pos tags {signifance} with respect to not using pos tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: QUESTION 7.1\n",
    "print(\"--- training svm discarding closed-class words ---\") # QUESTION: do we not use POS here??\n",
    "SVM=SVMText(bigrams=True,trigrams=False,discard_closed_class=True)\n",
    "SVM.crossValidate(corpus)\n",
    "svm_preds_closed=SVM.predictions\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.2f}\")\n",
    "p_value=signTest.getSignificance(svm_preds_closed,svm_preds)\n",
    "signifance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results discarding closed class {signifance} with respect to keeping the closed class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 8.0\n",
    "print(\"--- using document embeddings ---\")\n",
    "# load in IMDB dataset [TODO!!]\n",
    "corpus = None\n",
    "\n",
    "# train doc2vec\n",
    "feature_dim = 50\n",
    "window = 2\n",
    "epochs = 2\n",
    "DF = DocFeaturizer(feature_dim, window)\n",
    "DF.train(corpus, epochs)\n",
    "\n",
    "# apply to new prediction problem\n",
    "normalize_vecs = False\n",
    "svm_doc2vec_preds \n",
    "\n",
    "# compare to past models\n",
    "# need to re-train Naive Bayes on this dataset! \n",
    "print(f\"Accuracy: {SVMDV.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {SVMDV.getStdDeviation():.2f}\")\n",
    "p_value=signTest.getSignificance(svm_doc2vec_preds,smoothed_preds)\n",
    "signifance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results for SVM with  {signifance} with respect to Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze returned features and make a tSNE "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
