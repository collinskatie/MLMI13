{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os, codecs, sys\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# import numpy as np\n",
    "\n",
    "# data_dir = f\"data/aclImdb/\"\n",
    "# sent_dir = f\"{data_dir}test/pos/\"\n",
    "# all_reviews = [rev for rev in os.listdir(sent_dir) if rev[-4:] == \".txt\"]\n",
    "# all_reviews\n",
    "# fpth = f\"{sent_dir}{all_reviews[0]}\"\n",
    "# with open(fpth) as f:\n",
    "#     # NOTE: assumes we want to split by space -- discuss!!\n",
    "#     # e.g., \"kick the bucket\" diff meaning than individ words, but sep by spaces\n",
    "#     full_review_data = f.readlines()[0].split(\" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_review_data[0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kcollins/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import os, codecs, sys\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "data_dir = f\"data/aclImdb/\"\n",
    "sent_dir = f\"{data_dir}test/pos/\"\n",
    "all_reviews = [rev for rev in os.listdir(sent_dir) if rev[-4:] == \".txt\"]\n",
    "all_reviews\n",
    "fpth = f\"{sent_dir}{all_reviews[0]}\"\n",
    "with open(fpth) as f:\n",
    "    # NOTE: assumes we want to split by space -- discuss!!\n",
    "    # e.g., \"kick the bucket\" diff meaning than individ words, but sep by spaces\n",
    "    full_review_data = f.readlines()[0].split(\" \") \n",
    "    \n",
    "# data = [\"I love machine learning. Its awesome.\",\n",
    "#         \"I love coding in python\",\n",
    "#         \"I love building chatbots\",\n",
    "#         \"they chat amagingly well\"]\n",
    "\n",
    "tagged_data2 = [TaggedDocument(doc, [i]) for i, doc in enumerate([full_review_data])]\n",
    "\n",
    "data = [\" \".join(full_review_data)]\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00858445, -0.0025208 ,  0.00340851,  0.00559627, -0.00018348,\n",
       "       -0.00509644, -0.00727983,  0.00172328, -0.00224842,  0.00911236,\n",
       "        0.00550228,  0.00917563, -0.00229089, -0.00023794, -0.00128236,\n",
       "       -0.00012777, -0.00349216,  0.0084849 , -0.00831176, -0.00116774,\n",
       "        0.00279094, -0.00747453, -0.00042225, -0.00442204, -0.00958006,\n",
       "       -0.00708115,  0.00272685,  0.0046035 , -0.00072147,  0.00564601,\n",
       "       -0.00484888,  0.00538824, -0.0041378 , -0.00590575, -0.00047232,\n",
       "        0.00757254,  0.00792406,  0.00834035,  0.00814834,  0.00751941,\n",
       "       -0.00126159, -0.00535745,  0.00380593,  0.00660281,  0.00458905,\n",
       "       -0.00441384, -0.00312149, -0.00883249,  0.00029314,  0.00530691],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "model = Doc2Vec(vector_size=50, window=2)\n",
    "model.build_vocab(tagged_data2)\n",
    "model.train(tagged_data2, total_examples=model.corpus_count, epochs=2)\n",
    "model.infer_vector(full_review_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['POS',\n",
       "  ['based',\n",
       "   'on',\n",
       "   'an',\n",
       "   'actual',\n",
       "   'story,',\n",
       "   'john',\n",
       "   'boorman',\n",
       "   'shows',\n",
       "   'the',\n",
       "   'struggle',\n",
       "   'of',\n",
       "   'an',\n",
       "   'american',\n",
       "   'doctor,',\n",
       "   'whose',\n",
       "   'husband',\n",
       "   'and',\n",
       "   'son',\n",
       "   'were',\n",
       "   'murdered',\n",
       "   'and',\n",
       "   'she',\n",
       "   'was',\n",
       "   'continually',\n",
       "   'plagued',\n",
       "   'with',\n",
       "   'her',\n",
       "   'loss.',\n",
       "   'a',\n",
       "   'holiday',\n",
       "   'to',\n",
       "   'burma',\n",
       "   'with',\n",
       "   'her',\n",
       "   'sister',\n",
       "   'seemed',\n",
       "   'like',\n",
       "   'a',\n",
       "   'good',\n",
       "   'idea',\n",
       "   'to',\n",
       "   'get',\n",
       "   'away',\n",
       "   'from',\n",
       "   'it',\n",
       "   'all,',\n",
       "   'but',\n",
       "   'when',\n",
       "   'her',\n",
       "   'passport',\n",
       "   'was',\n",
       "   'stolen',\n",
       "   'in',\n",
       "   'rangoon,',\n",
       "   'she',\n",
       "   'could',\n",
       "   'not',\n",
       "   'leave',\n",
       "   'the',\n",
       "   'country',\n",
       "   'with',\n",
       "   'her',\n",
       "   'sister,',\n",
       "   'and',\n",
       "   'was',\n",
       "   'forced',\n",
       "   'to',\n",
       "   'stay',\n",
       "   'back',\n",
       "   'until',\n",
       "   'she',\n",
       "   'could',\n",
       "   'get',\n",
       "   'i.d.',\n",
       "   'papers',\n",
       "   'from',\n",
       "   'the',\n",
       "   'american',\n",
       "   'embassy.',\n",
       "   'to',\n",
       "   'fill',\n",
       "   'in',\n",
       "   'a',\n",
       "   'day',\n",
       "   'before',\n",
       "   'she',\n",
       "   'could',\n",
       "   'fly',\n",
       "   'out,',\n",
       "   'she',\n",
       "   'took',\n",
       "   'a',\n",
       "   'trip',\n",
       "   'into',\n",
       "   'the',\n",
       "   'countryside',\n",
       "   'with',\n",
       "   'a',\n",
       "   'tour',\n",
       "   'guide.',\n",
       "   '\"i',\n",
       "   'tried',\n",
       "   'finding',\n",
       "   'something',\n",
       "   'in',\n",
       "   'those',\n",
       "   'stone',\n",
       "   'statues,',\n",
       "   'but',\n",
       "   'nothing',\n",
       "   'stirred',\n",
       "   'in',\n",
       "   'me.',\n",
       "   'i',\n",
       "   'was',\n",
       "   'stone',\n",
       "   'myself.\"',\n",
       "   '<br',\n",
       "   '/><br',\n",
       "   '/>suddenly',\n",
       "   'all',\n",
       "   'hell',\n",
       "   'broke',\n",
       "   'loose',\n",
       "   'and',\n",
       "   'she',\n",
       "   'was',\n",
       "   'caught',\n",
       "   'in',\n",
       "   'a',\n",
       "   'political',\n",
       "   'revolt.',\n",
       "   'just',\n",
       "   'when',\n",
       "   'it',\n",
       "   'looked',\n",
       "   'like',\n",
       "   'she',\n",
       "   'had',\n",
       "   'escaped',\n",
       "   'and',\n",
       "   'safely',\n",
       "   'boarded',\n",
       "   'a',\n",
       "   'train,',\n",
       "   'she',\n",
       "   'saw',\n",
       "   'her',\n",
       "   'tour',\n",
       "   'guide',\n",
       "   'get',\n",
       "   'beaten',\n",
       "   'and',\n",
       "   'shot.',\n",
       "   'in',\n",
       "   'a',\n",
       "   'split',\n",
       "   'second',\n",
       "   'she',\n",
       "   'decided',\n",
       "   'to',\n",
       "   'jump',\n",
       "   'from',\n",
       "   'the',\n",
       "   'moving',\n",
       "   'train',\n",
       "   'and',\n",
       "   'try',\n",
       "   'to',\n",
       "   'rescue',\n",
       "   'him,',\n",
       "   'with',\n",
       "   'no',\n",
       "   'thought',\n",
       "   'of',\n",
       "   'herself.',\n",
       "   'continually',\n",
       "   'her',\n",
       "   'life',\n",
       "   'was',\n",
       "   'in',\n",
       "   'danger.',\n",
       "   '<br',\n",
       "   '/><br',\n",
       "   '/>here',\n",
       "   'is',\n",
       "   'a',\n",
       "   'woman',\n",
       "   'who',\n",
       "   'demonstrated',\n",
       "   'spontaneous,',\n",
       "   'selfless',\n",
       "   'charity,',\n",
       "   'risking',\n",
       "   'her',\n",
       "   'life',\n",
       "   'to',\n",
       "   'save',\n",
       "   'another.',\n",
       "   'patricia',\n",
       "   'arquette',\n",
       "   'is',\n",
       "   'beautiful,',\n",
       "   'and',\n",
       "   'not',\n",
       "   'just',\n",
       "   'to',\n",
       "   'look',\n",
       "   'at;',\n",
       "   'she',\n",
       "   'has',\n",
       "   'a',\n",
       "   'beautiful',\n",
       "   'heart.',\n",
       "   'this',\n",
       "   'is',\n",
       "   'an',\n",
       "   'unforgettable',\n",
       "   'story.',\n",
       "   '<br',\n",
       "   '/><br',\n",
       "   '/>\"we',\n",
       "   'are',\n",
       "   'taught',\n",
       "   'that',\n",
       "   'suffering',\n",
       "   'is',\n",
       "   'the',\n",
       "   'one',\n",
       "   'promise',\n",
       "   'that',\n",
       "   'life',\n",
       "   'always',\n",
       "   'keeps.\"']],\n",
       " ['POS',\n",
       "  ['this',\n",
       "   'is',\n",
       "   'a',\n",
       "   'gem.',\n",
       "   'as',\n",
       "   'a',\n",
       "   'film',\n",
       "   'four',\n",
       "   'production',\n",
       "   '-',\n",
       "   'the',\n",
       "   'anticipated',\n",
       "   'quality',\n",
       "   'was',\n",
       "   'indeed',\n",
       "   'delivered.',\n",
       "   'shot',\n",
       "   'with',\n",
       "   'great',\n",
       "   'style',\n",
       "   'that',\n",
       "   'reminded',\n",
       "   'me',\n",
       "   'some',\n",
       "   'errol',\n",
       "   'morris',\n",
       "   'films,',\n",
       "   'well',\n",
       "   'arranged',\n",
       "   'and',\n",
       "   'simply',\n",
       "   'gripping.',\n",
       "   \"it's\",\n",
       "   'long',\n",
       "   'yet',\n",
       "   'horrifying',\n",
       "   'to',\n",
       "   'the',\n",
       "   'point',\n",
       "   \"it's\",\n",
       "   'excruciating.',\n",
       "   'we',\n",
       "   'know',\n",
       "   'something',\n",
       "   'bad',\n",
       "   'happened',\n",
       "   '(one',\n",
       "   'can',\n",
       "   'guess',\n",
       "   'by',\n",
       "   'the',\n",
       "   'lack',\n",
       "   'of',\n",
       "   'participation',\n",
       "   'of',\n",
       "   'a',\n",
       "   'person',\n",
       "   'in',\n",
       "   'the',\n",
       "   'interviews)',\n",
       "   'but',\n",
       "   'we',\n",
       "   'are',\n",
       "   'compelled',\n",
       "   'to',\n",
       "   'see',\n",
       "   'it,',\n",
       "   'a',\n",
       "   'bit',\n",
       "   'like',\n",
       "   'a',\n",
       "   'car',\n",
       "   'accident',\n",
       "   'in',\n",
       "   'slow',\n",
       "   'motion.',\n",
       "   'the',\n",
       "   'story',\n",
       "   'spans',\n",
       "   'most',\n",
       "   'conceivable',\n",
       "   'aspects',\n",
       "   'and',\n",
       "   'unlike',\n",
       "   'some',\n",
       "   'documentaries',\n",
       "   'did',\n",
       "   'not',\n",
       "   'try',\n",
       "   'and',\n",
       "   'refrain',\n",
       "   'from',\n",
       "   'showing',\n",
       "   'the',\n",
       "   'grimmer',\n",
       "   'sides',\n",
       "   'of',\n",
       "   'the',\n",
       "   'stories,',\n",
       "   'as',\n",
       "   'also',\n",
       "   'dealing',\n",
       "   'with',\n",
       "   'the',\n",
       "   'guilt',\n",
       "   'of',\n",
       "   'the',\n",
       "   'people',\n",
       "   'don',\n",
       "   'left',\n",
       "   'behind',\n",
       "   'him,',\n",
       "   'wondering',\n",
       "   'why',\n",
       "   'they',\n",
       "   \"didn't\",\n",
       "   'stop',\n",
       "   'him',\n",
       "   'in',\n",
       "   'time.',\n",
       "   'it',\n",
       "   'took',\n",
       "   'me',\n",
       "   'a',\n",
       "   'few',\n",
       "   'hours',\n",
       "   'to',\n",
       "   'get',\n",
       "   'out',\n",
       "   'of',\n",
       "   'the',\n",
       "   'melancholy',\n",
       "   'that',\n",
       "   'gripped',\n",
       "   'me',\n",
       "   'after',\n",
       "   'seeing',\n",
       "   'this',\n",
       "   'very-well',\n",
       "   'made',\n",
       "   'documentary.']],\n",
       " ['POS',\n",
       "  ['i',\n",
       "   'really',\n",
       "   'like',\n",
       "   'this',\n",
       "   'show.',\n",
       "   'it',\n",
       "   'has',\n",
       "   'drama,',\n",
       "   'romance,',\n",
       "   'and',\n",
       "   'comedy',\n",
       "   'all',\n",
       "   'rolled',\n",
       "   'into',\n",
       "   'one.',\n",
       "   'i',\n",
       "   'am',\n",
       "   '28',\n",
       "   'and',\n",
       "   'i',\n",
       "   'am',\n",
       "   'a',\n",
       "   'married',\n",
       "   'mother,',\n",
       "   'so',\n",
       "   'i',\n",
       "   'can',\n",
       "   'identify',\n",
       "   'both',\n",
       "   'with',\n",
       "   \"lorelei's\",\n",
       "   'and',\n",
       "   \"rory's\",\n",
       "   'experiences',\n",
       "   'in',\n",
       "   'the',\n",
       "   'show.',\n",
       "   'i',\n",
       "   'have',\n",
       "   'been',\n",
       "   'watching',\n",
       "   'mostly',\n",
       "   'the',\n",
       "   'repeats',\n",
       "   'on',\n",
       "   'the',\n",
       "   'family',\n",
       "   'channel',\n",
       "   'lately,',\n",
       "   'so',\n",
       "   'i',\n",
       "   'am',\n",
       "   'not',\n",
       "   'up-to-date',\n",
       "   'on',\n",
       "   'what',\n",
       "   'is',\n",
       "   'going',\n",
       "   'on',\n",
       "   'now.',\n",
       "   'i',\n",
       "   'think',\n",
       "   'females',\n",
       "   'would',\n",
       "   'like',\n",
       "   'this',\n",
       "   'show',\n",
       "   'more',\n",
       "   'than',\n",
       "   'males,',\n",
       "   'but',\n",
       "   'i',\n",
       "   'know',\n",
       "   'some',\n",
       "   'men',\n",
       "   'out',\n",
       "   'there',\n",
       "   'would',\n",
       "   'enjoy',\n",
       "   'it!',\n",
       "   'i',\n",
       "   'really',\n",
       "   'like',\n",
       "   'that',\n",
       "   'is',\n",
       "   'an',\n",
       "   'hour',\n",
       "   'long',\n",
       "   'and',\n",
       "   'not',\n",
       "   'a',\n",
       "   'half',\n",
       "   'hour,',\n",
       "   'as',\n",
       "   'th',\n",
       "   'hour',\n",
       "   'seems',\n",
       "   'to',\n",
       "   'fly',\n",
       "   'by',\n",
       "   'when',\n",
       "   'i',\n",
       "   'am',\n",
       "   'watching',\n",
       "   'it!',\n",
       "   'give',\n",
       "   'it',\n",
       "   'a',\n",
       "   'chance',\n",
       "   'if',\n",
       "   'you',\n",
       "   'have',\n",
       "   'never',\n",
       "   'seen',\n",
       "   'the',\n",
       "   'show!',\n",
       "   'i',\n",
       "   'think',\n",
       "   'lorelei',\n",
       "   'and',\n",
       "   'luke',\n",
       "   'are',\n",
       "   'my',\n",
       "   'favorite',\n",
       "   'characters',\n",
       "   'on',\n",
       "   'the',\n",
       "   'show',\n",
       "   'though,',\n",
       "   'mainly',\n",
       "   'because',\n",
       "   'of',\n",
       "   'the',\n",
       "   'way',\n",
       "   'they',\n",
       "   'are',\n",
       "   'with',\n",
       "   'one',\n",
       "   'another.',\n",
       "   'how',\n",
       "   'could',\n",
       "   'you',\n",
       "   'not',\n",
       "   'see',\n",
       "   'something',\n",
       "   'was',\n",
       "   'there',\n",
       "   '(or',\n",
       "   'take',\n",
       "   'that',\n",
       "   'long',\n",
       "   'to',\n",
       "   'see',\n",
       "   'it',\n",
       "   'i',\n",
       "   'guess',\n",
       "   'i',\n",
       "   'should',\n",
       "   'say)?',\n",
       "   '<br',\n",
       "   '/><br',\n",
       "   '/>happy',\n",
       "   'viewing!']],\n",
       " ['POS',\n",
       "  ['this',\n",
       "   'is',\n",
       "   'the',\n",
       "   'best',\n",
       "   '3-d',\n",
       "   'experience',\n",
       "   'disney',\n",
       "   'has',\n",
       "   'at',\n",
       "   'their',\n",
       "   'themeparks.',\n",
       "   'this',\n",
       "   'is',\n",
       "   'certainly',\n",
       "   'better',\n",
       "   'than',\n",
       "   'their',\n",
       "   'original',\n",
       "   \"1960's\",\n",
       "   'acid-trip',\n",
       "   'film',\n",
       "   'that',\n",
       "   'was',\n",
       "   'in',\n",
       "   \"it's\",\n",
       "   'place,',\n",
       "   'is',\n",
       "   'leagues',\n",
       "   'better',\n",
       "   'than',\n",
       "   '\"honey',\n",
       "   'i',\n",
       "   'shrunk',\n",
       "   'the',\n",
       "   'audience\"',\n",
       "   '(and',\n",
       "   'far',\n",
       "   'more',\n",
       "   'fun),',\n",
       "   'barely',\n",
       "   'squeaks',\n",
       "   'by',\n",
       "   'the',\n",
       "   'muppetvision',\n",
       "   '3-d',\n",
       "   'movie',\n",
       "   'at',\n",
       "   'disney-mgm',\n",
       "   'and',\n",
       "   'can',\n",
       "   'even',\n",
       "   'beat',\n",
       "   'the',\n",
       "   'original',\n",
       "   '3-d',\n",
       "   '\"movie',\n",
       "   'experience\"',\n",
       "   'captain',\n",
       "   'eo.',\n",
       "   'this',\n",
       "   'film',\n",
       "   'relives',\n",
       "   'some',\n",
       "   'of',\n",
       "   \"disney's\",\n",
       "   'greatest',\n",
       "   'musical',\n",
       "   'hits',\n",
       "   'from',\n",
       "   'aladdin,',\n",
       "   'the',\n",
       "   'little',\n",
       "   'mermaid,',\n",
       "   'and',\n",
       "   'others,',\n",
       "   'and',\n",
       "   'brought',\n",
       "   'a',\n",
       "   'smile',\n",
       "   'to',\n",
       "   'my',\n",
       "   'face',\n",
       "   'throughout',\n",
       "   'the',\n",
       "   'entire',\n",
       "   'show.',\n",
       "   'this',\n",
       "   'is',\n",
       "   'a',\n",
       "   'totally',\n",
       "   'kid-friendly',\n",
       "   'movie',\n",
       "   'too,',\n",
       "   'unlike',\n",
       "   '\"honey...\"',\n",
       "   'and',\n",
       "   'has',\n",
       "   'more',\n",
       "   'effects',\n",
       "   'than',\n",
       "   'the',\n",
       "   'spectacular',\n",
       "   '\"muppetvision\"']],\n",
       " ['POS',\n",
       "  ['of',\n",
       "   'the',\n",
       "   'korean',\n",
       "   'movies',\n",
       "   \"i've\",\n",
       "   'seen,',\n",
       "   'only',\n",
       "   'three',\n",
       "   'had',\n",
       "   'really',\n",
       "   'stuck',\n",
       "   'with',\n",
       "   'me.',\n",
       "   'the',\n",
       "   'first',\n",
       "   'is',\n",
       "   'the',\n",
       "   'excellent',\n",
       "   'horror',\n",
       "   'a',\n",
       "   'tale',\n",
       "   'of',\n",
       "   'two',\n",
       "   'sisters.',\n",
       "   'the',\n",
       "   'second',\n",
       "   'and',\n",
       "   'third',\n",
       "   '-',\n",
       "   'and',\n",
       "   'now',\n",
       "   'fourth',\n",
       "   'too',\n",
       "   '-',\n",
       "   'have',\n",
       "   'all',\n",
       "   'been',\n",
       "   'park',\n",
       "   'chan',\n",
       "   \"wook's\",\n",
       "   'movies,',\n",
       "   'namely',\n",
       "   'oldboy,',\n",
       "   'sympathy',\n",
       "   'for',\n",
       "   'lady',\n",
       "   'vengeance),',\n",
       "   'and',\n",
       "   'now',\n",
       "   'thirst.',\n",
       "   '<br',\n",
       "   '/><br',\n",
       "   '/>park',\n",
       "   'kinda',\n",
       "   'reminds',\n",
       "   'me',\n",
       "   'of',\n",
       "   'quentin',\n",
       "   'tarantino',\n",
       "   'with',\n",
       "   'his',\n",
       "   'irreverence',\n",
       "   'towards',\n",
       "   'convention.',\n",
       "   'all',\n",
       "   'his',\n",
       "   'movies',\n",
       "   'are',\n",
       "   'shocking,',\n",
       "   'but',\n",
       "   'not',\n",
       "   'in',\n",
       "   'a',\n",
       "   'gratuitous',\n",
       "   'sense.',\n",
       "   \"it's\",\n",
       "   'more',\n",
       "   'like',\n",
       "   'he',\n",
       "   'shows',\n",
       "   'us',\n",
       "   'what',\n",
       "   'we',\n",
       "   \"don't\",\n",
       "   'expect',\n",
       "   'to',\n",
       "   'see',\n",
       "   '-',\n",
       "   'typically',\n",
       "   'situations',\n",
       "   'that',\n",
       "   'go',\n",
       "   'radically',\n",
       "   'against',\n",
       "   \"society's\",\n",
       "   'morals,',\n",
       "   'like',\n",
       "   'incest',\n",
       "   'or',\n",
       "   'a',\n",
       "   'libidinous,',\n",
       "   'blood-sucking,',\n",
       "   'yet',\n",
       "   'devout',\n",
       "   'priest.',\n",
       "   \"he's\",\n",
       "   'also',\n",
       "   'quite',\n",
       "   'artistically-inclined',\n",
       "   'with',\n",
       "   'regards',\n",
       "   'to',\n",
       "   'cinematography,',\n",
       "   'and',\n",
       "   'his',\n",
       "   'movies',\n",
       "   'are',\n",
       "   'among',\n",
       "   'the',\n",
       "   'more',\n",
       "   'gorgeous',\n",
       "   'that',\n",
       "   \"i've\",\n",
       "   'seen.<br',\n",
       "   '/><br',\n",
       "   '/>thirst',\n",
       "   'is',\n",
       "   'all',\n",
       "   'that',\n",
       "   '-',\n",
       "   'being',\n",
       "   'about',\n",
       "   'said',\n",
       "   'priest',\n",
       "   'and',\n",
       "   'the',\n",
       "   'repressed,',\n",
       "   'conscience-less',\n",
       "   'woman',\n",
       "   'he',\n",
       "   'falls',\n",
       "   'for',\n",
       "   '-',\n",
       "   'and',\n",
       "   'more.',\n",
       "   \"it's\",\n",
       "   'horror,',\n",
       "   'drama,',\n",
       "   'and',\n",
       "   'even',\n",
       "   'comedy,',\n",
       "   'as',\n",
       "   'park',\n",
       "   'disarms',\n",
       "   'his',\n",
       "   'audience',\n",
       "   'with',\n",
       "   'many',\n",
       "   'inappropriate',\n",
       "   'yet',\n",
       "   'humorous',\n",
       "   'situations.',\n",
       "   'as',\n",
       "   'such,',\n",
       "   'this',\n",
       "   'might',\n",
       "   'be',\n",
       "   'his',\n",
       "   'best',\n",
       "   'work',\n",
       "   'for',\n",
       "   'me',\n",
       "   'yet,',\n",
       "   'since',\n",
       "   'his',\n",
       "   'other',\n",
       "   'two',\n",
       "   'movies',\n",
       "   'that',\n",
       "   \"i've\",\n",
       "   'seen',\n",
       "   'were',\n",
       "   'lacking',\n",
       "   'the',\n",
       "   'humor',\n",
       "   'element',\n",
       "   'that',\n",
       "   \"would've\",\n",
       "   'made',\n",
       "   'them',\n",
       "   'more',\n",
       "   'palatable',\n",
       "   'for',\n",
       "   'repeat',\n",
       "   'viewings.']]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train: 1800, num test: 200\n",
      "tot num reviews: 2000\n",
      "--- classifying reviews using sentiment lexicon  ---\n",
      "--- using document embeddings ---\n",
      "num train: 25000, num test: 25000\n",
      "tot num reviews: 50000\n"
     ]
    }
   ],
   "source": [
    "from Corpora import MovieReviewCorpus\n",
    "from Lexicon import SentimentLexicon\n",
    "from Statistics import SignTest\n",
    "from Classifiers import NaiveBayesText, SVMText\n",
    "from Extensions import SVMDoc2Vec, DocFeaturizer\n",
    "\n",
    "\n",
    "# retrieve corpus\n",
    "corpus=MovieReviewCorpus(stemming=False,pos=False)\n",
    "\n",
    "# use sign test for all significance testing\n",
    "signTest=SignTest()\n",
    "\n",
    "print(\"--- classifying reviews using sentiment lexicon  ---\")\n",
    "\n",
    "# read in lexicon\n",
    "lexicon=SentimentLexicon()\n",
    "\n",
    "# on average there are more positive than negative words per review (~7.13 more positive than negative per review)\n",
    "# to take this bias into account will use threshold (roughly the bias itself) to make it harder to classify as positive\n",
    "# todo: vary this!!! \n",
    "threshold=8\n",
    "\n",
    "# question 8.0\n",
    "print(\"--- using document embeddings ---\")\n",
    "# load in IMDB dataset [TODO!!]\n",
    "corpus=MovieReviewCorpus(stemming=False,pos=False,use_imdb=True)\n",
    "\n",
    "# train doc2vec\n",
    "feature_dim = 200\n",
    "window = 5\n",
    "epochs = 100\n",
    "DF = DocFeaturizer(feature_dim, window)\n",
    "DF.train_model(corpus.train, epochs)\n",
    "\n",
    "SVMDV=SVMDoc2Vec(model=DF,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "SVMDV.train(corpus.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMDV.test(corpus.test)\n",
    "svm_doc2vec_preds=SVMDV.predictions\n",
    "print(f\"Accuracy: {SVMDV.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {SVMDV.getStdDeviation():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embs = DF.get_embeddings(corpus.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embs = DF.get_embeddings(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__init__() takes exactly one argument (the instance to initialize)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-82f076fa3973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSVM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVMDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrigrams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscard_closed_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLMI13/Extensions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, bigrams, trigrams, discard_closed_class, normalize_vecs)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mrandom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \"\"\"\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVMText\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrigrams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscard_closed_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object.__init__() takes exactly one argument (the instance to initialize)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POS'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply to new prediction problem\n",
    "normalize_vecs = False\n",
    "svm_doc2vec_preds \n",
    "\n",
    "# compare to past models\n",
    "# need to re-train Naive Bayes on this dataset! \n",
    "print(f\"Accuracy: {SVMDV.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {SVMDV.getStdDeviation():.2f}\")\n",
    "p_value=signTest.getSignificance(svm_doc2vec_preds,smoothed_preds)\n",
    "signifance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results for SVM with  {signifance} with respect to Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Corpora import MovieReviewCorpus\n",
    "from Lexicon import SentimentLexicon\n",
    "from Statistics import SignTest\n",
    "from Classifiers import NaiveBayesText, SVMText\n",
    "from Extensions import SVMDoc2Vec, DocFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve corpus\n",
    "corpus=MovieReviewCorpus(stemming=False,pos=False)\n",
    "\n",
    "# use sign test for all significance testing\n",
    "signTest=SignTest()\n",
    "\n",
    "print(\"--- classifying reviews using sentiment lexicon  ---\")\n",
    "\n",
    "# read in lexicon\n",
    "lexicon=SentimentLexicon()\n",
    "\n",
    "# on average there are more positive than negative words per review (~7.13 more positive than negative per review)\n",
    "# to take this bias into account will use threshold (roughly the bias itself) to make it harder to classify as positive\n",
    "# todo: vary this!!! \n",
    "threshold=8\n",
    "\n",
    "# NOTE: from katie to self -- play w/ changing the threshold value! \n",
    "\n",
    "# question 0.1\n",
    "lexicon.classify(corpus.reviews,threshold,magnitude=False)\n",
    "token_preds=lexicon.predictions\n",
    "print(f\"token-only results: {lexicon.getAccuracy():.2f}\")\n",
    "\n",
    "lexicon.classify(corpus.reviews,threshold,magnitude=True)\n",
    "magnitude_preds=lexicon.predictions\n",
    "print(f\"magnitude results: {lexicon.getAccuracy():.2f}\")\n",
    "\n",
    "# question 0.2\n",
    "p_value=signTest.getSignificance(token_preds,magnitude_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"magnitude lexicon results are {significance} with respect to token-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 1.0\n",
    "print(\"--- classifying reviews using Naive Bayes on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus.train)\n",
    "NB.test(corpus.test)\n",
    "# store predictions from classifier\n",
    "non_smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy without smoothing: {NB.getAccuracy():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2.0\n",
    "# use smoothing\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus.train)\n",
    "NB.test(corpus.test)\n",
    "smoothed_preds=NB.predictions\n",
    "# saving this for use later\n",
    "num_non_stemmed_features=len(NB.vocabulary)\n",
    "print(f\"Accuracy using smoothing: {NB.getAccuracy():.2f}\")\n",
    "\n",
    "# question 2.1\n",
    "# see if smoothing significantly improves results\n",
    "p_value=signTest.getSignificance(non_smoothed_preds,smoothed_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing are {significance} with respect to no smoothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3.0\n",
    "print(\"--- classifying reviews using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus)\n",
    "# using cross-eval for smoothed predictions from now on\n",
    "smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.3f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 4.0\n",
    "print(\"--- stemming corpus ---\")\n",
    "# retrieve corpus with tokenized text and stemming (using porter)\n",
    "stemmed_corpus=MovieReviewCorpus(stemming=True,pos=False)\n",
    "print(\"--- cross-validating NB using stemming ---\")\n",
    "NB.crossValidate(stemmed_corpus)\n",
    "stemmed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.3f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.3f}\")\n",
    "\n",
    "# TODO Q4.1\n",
    "# see if stemming significantly improves results on smoothed NB (both did cv)\n",
    "p_value=signTest.getSignificance(stemmed_preds,smoothed_preds) # note compared against version w/ smoothing! \n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using stemming are {significance} with respect to no stemming\")\n",
    "\n",
    "# TODO Q4.2\n",
    "print(\"--- determining the number of features before/after stemming ---\")\n",
    "# (**) changing the number of features == changing the number of words in the vocab \n",
    "NB.train(corpus.train)\n",
    "NB.test(corpus.test)\n",
    "num_stemmed_features = len(NB.vocabulary)\n",
    "print(f\"num features, non-stemmed: {num_non_stemmed_features} vs. num features, stemmed: {num_stemmed_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question Q5.0\n",
    "# cross-validate model using smoothing and bigrams\n",
    "print(\"--- cross-validating naive bayes using smoothing and bigrams ---\")\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=True,trigrams=False,discard_closed_class=False)\n",
    "NB.crossValidate(corpus)\n",
    "smoothed_and_bigram_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.2f}\")\n",
    "\n",
    "\n",
    "# see if bigrams significantly improves results on smoothed NB only\n",
    "p_value=signTest.getSignificance(smoothed_preds,smoothed_and_bigram_preds)\n",
    "signifance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing and bigrams are {signifance} with respect to smoothing only\")\n",
    "\n",
    "\n",
    "# TODO Q5.1\n",
    "# katie: from q3 (num_non_stemmed_features)\n",
    "num_bow_features = len(NB.vocabulary)\n",
    "print(f\"num features for [model] (Q3): {num_non_stemmed_features} vs. num features BoW: {num_bow_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q6 and 6.1\n",
    "print(\"--- classifying reviews using SVM 10-fold cross-eval ---\")\n",
    "SVM=SVMText(bigrams=True,trigrams=False,discard_closed_class=False)\n",
    "SVM.crossValidate(corpus)\n",
    "svm_preds=SVM.predictions\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Q7.0\n",
    "print(\"--- adding in POS information to corpus ---\")\n",
    "pos_corpus=MovieReviewCorpus(stemming=False,pos=True)\n",
    "print(\"--- training svm on word+pos features ----\")\n",
    "SVM=SVMText(bigrams=True,trigrams=False,discard_closed_class=False)\n",
    "SVM.crossValidate(pos_corpus)\n",
    "svm_pos_preds=SVM.predictions\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.2f}\")\n",
    "p_value=signTest.getSignificance(svm_pos_preds,svm_preds)\n",
    "signifance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using pos tags {signifance} with respect to not using pos tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: QUESTION 7.1\n",
    "print(\"--- training svm discarding closed-class words ---\") # QUESTION: do we not use POS here??\n",
    "SVM=SVMText(bigrams=True,trigrams=False,discard_closed_class=True)\n",
    "SVM.crossValidate(corpus)\n",
    "svm_preds_closed=SVM.predictions\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.2f}\")\n",
    "p_value=signTest.getSignificance(svm_preds_closed,svm_preds)\n",
    "signifance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results discarding closed class {signifance} with respect to keeping the closed class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 8.0\n",
    "print(\"--- using document embeddings ---\")\n",
    "# load in IMDB dataset [TODO!!]\n",
    "corpus = None\n",
    "\n",
    "# train doc2vec\n",
    "feature_dim = 50\n",
    "window = 2\n",
    "epochs = 2\n",
    "DF = DocFeaturizer(feature_dim, window)\n",
    "DF.train(corpus, epochs)\n",
    "\n",
    "# apply to new prediction problem\n",
    "normalize_vecs = False\n",
    "svm_doc2vec_preds \n",
    "\n",
    "# compare to past models\n",
    "# need to re-train Naive Bayes on this dataset! \n",
    "print(f\"Accuracy: {SVMDV.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {SVMDV.getStdDeviation():.2f}\")\n",
    "p_value=signTest.getSignificance(svm_doc2vec_preds,smoothed_preds)\n",
    "signifance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results for SVM with  {signifance} with respect to Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze returned features and make a tSNE "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
